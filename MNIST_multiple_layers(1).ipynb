{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bc3164b-a8e4-4f13-8ca6-1e6b46abf02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Add path to the nxsdk_modules package.\n",
    "sys.path.append('/homes/dcicch/models')\n",
    "sys.path.append('/homes/dcicch/NxSDK_Package/nxsdk-apps')\n",
    "\n",
    "\n",
    "# Enable SLURM to run network on Loihi.\n",
    "os.environ['SLURM'] = '1'\n",
    "os.environ['PYTHONUNBUFFERED'] = '1'\n",
    "\n",
    "os.environ[\"PARTITION\"] = \"nahuku32_2h\"\n",
    "os.environ[\"BOARD\"] = 'ncl-ext-ghrd-01'\n",
    "\n",
    "snipDir = os.path.abspath(os.path.join('..', 'snips', 'reset_model_states'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5c6cdd3-90ba-4653-9b54-f713ea94feb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_training_epochs = 2\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# set later\n",
    "#num_steps_per_img = 75\n",
    "num_train_samples = 60000\n",
    "num_test_samples = 128\n",
    "\n",
    "# EnergyProbes allow to profile execution time, power and thus energy consumption\n",
    "enable_energy_probe = True\n",
    "# set later\n",
    "#execution_time_probe_bin_size = 75 # Small numbers slows down execution\n",
    "\n",
    "# Not yet supported\n",
    "measure_accuracy_runtime_trade_off = False # Not yet supported\n",
    "runtimes = [128, 256, 512, 1024, 2048]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3479f64c-9bfc-4b47-a1b4-f4469c4d92d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-22 17:01:31.591399: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-22 17:01:32.003692: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /slurm/intel-archi/lib\n",
      "2023-01-22 17:01:32.003724: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-22 17:01:34.036464: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /slurm/intel-archi/lib\n",
      "2023-01-22 17:01:34.036537: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /slurm/intel-archi/lib\n",
      "2023-01-22 17:01:34.036543: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Load standard MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize input so we can train ANN with it. \n",
    "# Will be converted back to integers for SNN layer.\n",
    "x_train = x_train[:num_train_samples, :, :] / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# Add a channel dimension.\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "# One-hot encode target vectors.\n",
    "y_train = np_utils.to_categorical(y_train[:num_train_samples], 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f972b2f0-fb36-4082-81be-f2f4b6602977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModel(num_steps_per_sample, x_test, y_test):\n",
    "    \"\"\"Runs the SNN Model to classify test images.\"\"\"\n",
    "      \n",
    "    # Initialize arrays for results\n",
    "    num_samples = len(y_test)\n",
    "    classifications = np.zeros(num_samples, int)\n",
    "    labels = np.zeros(num_samples, int)\n",
    "\n",
    "    # Run DNN to classify images\n",
    "    tStart = time.time()\n",
    "    snn_model.run(num_steps_per_sample * num_samples, aSync=True)\n",
    "    tEndBoot = time.time()\n",
    "\n",
    "    # Enqueue images by pushing them into InputGenerator\n",
    "    print(\"Queuing images...\")\n",
    "    for i, (x, y) in enumerate(zip((x_test* 255).astype(int), y_test)):\n",
    "        input_generator.encode(x)\n",
    "        labels[i] = np.argmax(y)\n",
    "    tEndInput = time.time()\n",
    "\n",
    "    # Read out classification results for all images\n",
    "    print(\"Waiting for classification to finish...\")\n",
    "    classifications = list(dnn.readout_channel.read(num_samples))\n",
    "    tEndClassification = time.time()\n",
    "    \n",
    "    # finishRun fetches EnergyProbe data if configured\n",
    "    snn_model.finishRun()\n",
    "    \n",
    "    return tStart, tEndBoot, tEndInput, tEndClassification, classifications, labels\n",
    "\n",
    "def calcAccuracy(classifications, labels):\n",
    "    \"\"\"Computes classification accuracy for a set of images given classification and labels.\"\"\"\n",
    "    errors = classifications != labels\n",
    "    num_errors = np.sum(errors)\n",
    "    num_samples = len(classifications)\n",
    "    return (num_samples-num_errors)/num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f207348e-8840-4551-b034-be55528b5ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(samples, guesses, lables, numCols=5, size=28):\n",
    "    \"\"\"Plots samples as an array of images.\"\"\"\n",
    "    \n",
    "    import math\n",
    "    numSamples = len(samples)\n",
    "    numRows = int(math.ceil(numSamples/numCols))\n",
    "    plt.figure(3, figsize=(20, 10))\n",
    "    i = 0\n",
    "    for c in range(numCols):\n",
    "        for r in range(numRows):\n",
    "            plt.subplot(numRows, numCols, i+1)\n",
    "            plt.imshow(np.reshape(samples[i,:], (size, size)))\n",
    "            plt.axis('off')\n",
    "            plt.title('C:{}/L:{}'.format(guesses[i], lables[i]))\n",
    "            i += 1\n",
    "            if i == numSamples:\n",
    "                break\n",
    "        if i == numSamples:\n",
    "            break\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34d22b41-7b60-493c-b1da-d991f3d8023a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "#from keras.models import Model\n",
    "from keras.layers import Dropout, Flatten, Conv2D, Input, Dense\n",
    "\n",
    "\n",
    "from nxsdk_modules_ncl.dnn.src.utils import extract\n",
    "from nxsdk_modules_ncl.dnn.src.dnn_layers import NxInputLayer, NxConv2D, NxDense, \\\n",
    "    NxModel, NxFlatten, ProbableStates\n",
    "\n",
    "from nxsdk_modules_ncl.dnn.src.optimization import ExclusionCriteria\n",
    "from nxsdk_modules_ncl.dnn.composable.composable_dnn import ComposableDNN\n",
    "\n",
    "from nxsdk_modules_ncl.dnn.src.utils import to_integer\n",
    "\n",
    "from nxsdk_modules_ncl.dnn.composable.composable_dnn import ComposableDNN as DNN\n",
    "from nxsdk_modules.input_generator.input_generator import InputGenerator\n",
    "\n",
    "from nxsdk.composable.model import Model\n",
    "from nxsdk.logutils.nxlogging import set_verbosity,LoggingLevel\n",
    "import csv\n",
    "#from nxsdk.api.n2a as \n",
    "set_verbosity(LoggingLevel.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44948f1f-2e6e-4b25-b3cd-46d635e74580",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import csv\n",
    "#\n",
    "#with open('t_acc_e_conv.csv', 'w') as csvfile:\n",
    "#    writer = csv.writer(csvfile)\n",
    "#    writer.writerows(vals_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6920b656-3efc-43bc-9219-d8b14a4b612c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of time steps: 10\n",
      "Neurons in hidden layer: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-22 17:01:44.001277: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /slurm/intel-archi/lib\n",
      "2023-01-22 17:01:44.006360: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-22 17:01:44.006373: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ncl-edu): /proc/driver/nvidia/version does not exist\n",
      "2023-01-22 17:01:44.006686: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1875/1875 - 4s - loss: 0.5038 - accuracy: 0.8595 - val_loss: 0.2971 - val_accuracy: 0.9186 - 4s/epoch - 2ms/step\n",
      "Epoch 2/2\n",
      "1875/1875 - 3s - loss: 0.2881 - accuracy: 0.9184 - val_loss: 0.2740 - val_accuracy: 0.9262 - 3s/epoch - 1ms/step\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28)]          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                7850      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,960\n",
      "Trainable params: 7,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/homes/dcicch/models/temp/1674435710.90408'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 152\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[39m# Enfore particular execution order or processes/snips executing in the same phase\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39m# (Here: Execute input injection as bias currents after network reset)\u001b[39;00m\n\u001b[1;32m    150\u001b[0m input_generator\u001b[39m.\u001b[39mprocesses\u001b[39m.\u001b[39minputEncoder\u001b[39m.\u001b[39mexecuteAfter(dnn\u001b[39m.\u001b[39mprocesses\u001b[39m.\u001b[39mreset)\n\u001b[0;32m--> 152\u001b[0m snn_model\u001b[39m.\u001b[39;49mcompile()\n\u001b[1;32m    154\u001b[0m \u001b[39mif\u001b[39;00m enable_energy_probe:\n\u001b[1;32m    155\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnxsdk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menums\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi_enums\u001b[39;00m \u001b[39mimport\u001b[39;00m ProbeParameter\n",
      "File \u001b[0;32m~/nengoloihi/miniconda/lib/python3.9/site-packages/nxsdk/composable/model.py:121\u001b[0m, in \u001b[0;36mModel.compile\u001b[0;34m(self, board, numCoresPerChip)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_board \u001b[39m=\u001b[39m N2Board(\n\u001b[1;32m    118\u001b[0m     \u001b[39mid\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, numCoresPerChip\u001b[39m=\u001b[39mnumCoresPerChip) \u001b[39mif\u001b[39;00m board \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m board\n\u001b[1;32m    120\u001b[0m \u001b[39m# Call partition, map and update ports with resource map for each composable\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compile(sortedComposables)\n\u001b[1;32m    123\u001b[0m \u001b[39m# Aggregate all processes, group them and create aggregated snips\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_processAggregator\u001b[39m.\u001b[39maggregateProcesses(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocesses, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_board)\n",
      "File \u001b[0;32m~/nengoloihi/miniconda/lib/python3.9/site-packages/nxsdk/composable/model.py:150\u001b[0m, in \u001b[0;36mModel._compile\u001b[0;34m(self, sortedComposables)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39m# Compilation pipeline\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[39mfor\u001b[39;00m composable \u001b[39min\u001b[39;00m sortedComposables:\n\u001b[1;32m    149\u001b[0m     \u001b[39m# Call partition, map and updatePorts on each\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m     composable\u001b[39m.\u001b[39;49mpartition(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_board)\\\n\u001b[1;32m    151\u001b[0m         \u001b[39m.\u001b[39;49mmap(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_board)\\\n\u001b[1;32m    152\u001b[0m         \u001b[39m.\u001b[39mupdatePorts(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_board)\n",
      "File \u001b[0;32m/homes/dcicch/models/nxsdk_modules_ncl/dnn/composable/composable_dnn.py:119\u001b[0m, in \u001b[0;36mComposableDNN.map\u001b[0;34m(self, board)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m, board: Graph) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m AbstractComposable:\n\u001b[1;32m    118\u001b[0m     \u001b[39m\"\"\"Invoke partition and mapping of the dnn model\"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     mapper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dnn\u001b[39m.\u001b[39;49mcompileModel(board)\n\u001b[1;32m    121\u001b[0m     printLayerInfo \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[39mif\u001b[39;00m printLayerInfo:\n",
      "File \u001b[0;32m/homes/dcicch/models/nxsdk_modules_ncl/dnn/src/dnn_layers.py:2924\u001b[0m, in \u001b[0;36mNxModel.compileModel\u001b[0;34m(self, board)\u001b[0m\n\u001b[1;32m   2915\u001b[0m \u001b[39m\"\"\"Compile the ``NxModel``.\u001b[39;00m\n\u001b[1;32m   2916\u001b[0m \n\u001b[1;32m   2917\u001b[0m \u001b[39mAttemps to load intermediate representations from disk. Continues\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2920\u001b[0m \u001b[39m:param N2Board | None board: Board where the model is mapped to.\u001b[39;00m\n\u001b[1;32m   2921\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2923\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_isInitialized:\n\u001b[0;32m-> 2924\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minitialize()\n\u001b[1;32m   2926\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mboard \u001b[39m=\u001b[39m N2Board(\u001b[39m0\u001b[39m, numCoresPerChip\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maxNumCoresPerChip) \\\n\u001b[1;32m   2927\u001b[0m     \u001b[39mif\u001b[39;00m board \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m board\n\u001b[1;32m   2929\u001b[0m hasPartitionConfig, hasMappable \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcanLoad()\n",
      "File \u001b[0;32m/homes/dcicch/models/nxsdk_modules_ncl/dnn/src/dnn_layers.py:2809\u001b[0m, in \u001b[0;36mNxModel.initialize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2805\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogdir \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2806\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtmpdir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(\n\u001b[1;32m   2807\u001b[0m         os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mrealpath(\u001b[39m__file__\u001b[39m)),\n\u001b[1;32m   2808\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m../../..\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtemp\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mstr\u001b[39m(time\u001b[39m.\u001b[39mtime())))\n\u001b[0;32m-> 2809\u001b[0m     os\u001b[39m.\u001b[39;49mmakedirs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtmpdir, exist_ok\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   2810\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogdir \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtmpdir\n\u001b[1;32m   2812\u001b[0m modelDumpDir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogdir, \u001b[39m'\u001b[39m\u001b[39mmodel_dumps\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/nengoloihi/miniconda/lib/python3.9/os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     mkdir(name, mode)\n\u001b[1;32m    226\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     \u001b[39m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[39m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m exist_ok \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m path\u001b[39m.\u001b[39misdir(name):\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/homes/dcicch/models/temp/1674435710.90408'"
     ]
    }
   ],
   "source": [
    "## timing ANN\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "class TimingCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, logs={}):\n",
    "        self.logs=[]\n",
    "    def on_test_begin(self, logs={}):\n",
    "        self.starttime = timer()\n",
    "    def on_test_end(self, logs={}):\n",
    "        self.logs.append(timer()-self.starttime)\n",
    "\n",
    "train_model = True\n",
    "     \n",
    "## Array for time\n",
    "## Array for accuracy\n",
    "## Array for Energy\n",
    "vals_dense = np.zeros((5, 5, 10))\n",
    "    \n",
    "    \n",
    "# Path for pre-trained model\n",
    "pretrained_model_path = os.path.join(os.path.abspath(''),\n",
    "                                     'models', \n",
    "                                     'a_minist_model.h5')\n",
    "\n",
    "layer_sizes = [10, 40, 100, 400, 1000, 4000]\n",
    "step_sizes = [10, 25, 100, 25, 1000]\n",
    "#layer_sizes = [10]\n",
    "#step_sizes = [10]\n",
    "\n",
    "ann_time = np.zeros((len(layer_sizes), 10))\n",
    "accuracy_ANN = np.zeros((len(layer_sizes), 10))\n",
    "vals_dense = np.zeros((5, len(layer_sizes), len(step_sizes), 10))\n",
    "\n",
    "mean_vals_t1 = np.zeros((len(layer_sizes), len(step_sizes)))\n",
    "std_vals_t1 = np.zeros((len(layer_sizes), len(step_sizes)))\n",
    "mean_vals_t2 = np.zeros((len(layer_sizes), len(step_sizes)))\n",
    "std_vals_t2 = np.zeros((len(layer_sizes), len(step_sizes)))\n",
    "mean_vals_acc = np.zeros((len(layer_sizes), len(step_sizes)))\n",
    "std_vals_acc = np.zeros((len(layer_sizes), len(step_sizes)))\n",
    "mean_vals_e1 = np.zeros((len(layer_sizes), len(step_sizes)))\n",
    "std_vals_e1 = np.zeros((len(layer_sizes), len(step_sizes)))\n",
    "mean_vals_e2 = np.zeros((len(layer_sizes), len(step_sizes)))\n",
    "std_vals_e2 = np.zeros((len(layer_sizes), len(step_sizes)))\n",
    "\n",
    "\n",
    "n_dim = 10\n",
    "    \n",
    "# Generate model\n",
    "for k in range(len(step_sizes)):\n",
    "    print(f\"Number of time steps: {step_sizes[k]}\")\n",
    "    num_steps_per_img = step_sizes[k]\n",
    "    execution_time_probe_bin_size = step_sizes[k]\n",
    "    for i in range(len(layer_sizes)):\n",
    "        print(f\"Neurons in hidden layer: {layer_sizes[i]}\")\n",
    "        for j in range(10):\n",
    "            if train_model or not os.path.isfile(pretrained_model_path):\n",
    "                # Define model\n",
    "                input_layer = Input(shape=(28,28))\n",
    "\n",
    "                input_layer_flat = Flatten()(input_layer)\n",
    "\n",
    "                layer = Dense(layer_sizes[i], activation='relu')(input_layer_flat)\n",
    "\n",
    "                outputs = Dense(10, activation='softmax')(layer)\n",
    "\n",
    "                ann_model = keras.models.Model(input_layer, outputs)\n",
    "\n",
    "                ann_model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "                cb = TimingCallback()\n",
    "\n",
    "                # Training \n",
    "                history = ann_model.fit(x_train, y_train, batch_size, num_training_epochs, verbose=2,\n",
    "                      validation_data=(x_test, y_test), callbacks=[cb])\n",
    "\n",
    "                ann_time[i, j] = sum(cb.logs)\n",
    "                accuracy_ANN[i, j] = np.mean(history.history['val_accuracy'])\n",
    "\n",
    "                # Save model\n",
    "                #ann_model.save(pretrained_model_path)\n",
    "            else:\n",
    "                # Load pre-trained model\n",
    "                ann_model = keras.models.load_model(pretrained_model_path)\n",
    "\n",
    "            ann_model.summary()\n",
    "\n",
    "\n",
    "            vth_mant = 2**9\n",
    "            bias_exp = 6\n",
    "            weight_exponent = 0\n",
    "            synapse_encoding = 'sparse'\n",
    "            #exclusionCriteria = ExclusionCriteria()\n",
    "\n",
    "            inputLayer = NxInputLayer((28,28), \n",
    "                                         vThMant=vth_mant, \n",
    "                                         biasExp=bias_exp)\n",
    "\n",
    "            input_layer = NxFlatten()(inputLayer.input)\n",
    "\n",
    "            layer = NxDense(layer_sizes[i], activation='relu')(input_layer)\n",
    "\n",
    "            outputs = NxDense(10, activation='softmax')(layer)\n",
    "\n",
    "            snn_nxmodel = NxModel(inputLayer.input, outputs, numCandidatesToCompute=1)\n",
    "\n",
    "            #snn_nxmodel.summary()\n",
    "\n",
    "\n",
    "            # Extract weights and biases from parameter list.\n",
    "            parameters = ann_model.get_weights()\n",
    "            weights = parameters[0::2]\n",
    "            biases = parameters[1::2]\n",
    "\n",
    "            # Quantize weights and biases using max-normalization (Strong quantization loss if distributions have large tails)\n",
    "            parameters_int = []\n",
    "            for w, b in zip(weights, biases):\n",
    "                w_int, b_int = to_integer(w, b, 8)\n",
    "                parameters_int += [w_int, b_int]\n",
    "\n",
    "            # Set quantized weigths and biases for spiking model\n",
    "            snn_nxmodel.set_weights(parameters_int)\n",
    "\n",
    "\n",
    "            # NxModel is not yet implemented as a Composable -> Wrap it with DNN composable class\n",
    "            dnn = DNN(model=snn_nxmodel, num_steps_per_img=num_steps_per_img)\n",
    "\n",
    "            input_generator = InputGenerator(shape=(28,28), interval=num_steps_per_img)\n",
    "\n",
    "            input_generator.setBiasExp(bias_exp)\n",
    "\n",
    "\n",
    "            #set_verbosity(LoggingLevel.ERROR)\n",
    "\n",
    "            # Initialize empty model\n",
    "            snn_model = Model(\"dnn_model\")\n",
    "\n",
    "            # Add DNN and InputGenerator to empty model\n",
    "            snn_model.add(dnn)\n",
    "            snn_model.add(input_generator)\n",
    "\n",
    "\n",
    "            # Connect InputGenerator to DNN\n",
    "            # (Explicit)\n",
    "            # input_generator.ports.output.connect(dnn.ports.input)\n",
    "            # (Implicit when ports can be inferred)\n",
    "            input_generator.connect(dnn)\n",
    "\n",
    "            # Enfore particular execution order or processes/snips executing in the same phase\n",
    "            # (Here: Execute input injection as bias currents after network reset)\n",
    "            input_generator.processes.inputEncoder.executeAfter(dnn.processes.reset)\n",
    "\n",
    "            snn_model.compile()\n",
    "\n",
    "            if enable_energy_probe:\n",
    "                from nxsdk.api.enums.api_enums import ProbeParameter\n",
    "                from nxsdk.graph.monitor.probes import PerformanceProbeCondition\n",
    "                eProbe = snn_model.board.probe(probeType=ProbeParameter.ENERGY, \n",
    "                                               probeCondition=PerformanceProbeCondition(\n",
    "                                                    tStart=1, \n",
    "                                                    tEnd=num_test_samples*num_steps_per_img, \n",
    "                                                    bufferSize=1024, \n",
    "                                                    binSize=execution_time_probe_bin_size))\n",
    "\n",
    "            snn_model.start(snn_nxmodel.board)\n",
    "\n",
    "            tStart, tEndBoot, tEndInput, tEndClassification, classifications, labels = runModel(num_steps_per_img, \n",
    "                                                                                                x_test[:num_test_samples, :, :, 0], \n",
    "                                                                                                y_test[:num_test_samples])\n",
    "\n",
    "             # Runtime statistics\n",
    "            runtimeBoot = tEndBoot-tStart\n",
    "            runtimeInput = tEndInput-tStart\n",
    "            runtimeClassification = tEndClassification-tStart\n",
    "\n",
    "            vals_dense[0, i, k, j] = runtimeClassification-runtimeInput\n",
    "            vals_dense[3, i, k, j] = np.mean(eProbe.spikingTimePerTimeStep)*num_steps_per_img/1e3\n",
    "\n",
    "            # Accuracy statistics\n",
    "            errors = classifications != labels\n",
    "            num_errors = np.sum(errors)\n",
    "\n",
    "            vals_dense[1, i, k, j] = (num_test_samples-num_errors)/num_test_samples\n",
    "\n",
    "\n",
    "            vals_dense[2, i, k, j] = eProbe.totalEnergy\n",
    "            vals_dense[4, i, k, j] = np.mean(eProbe.totalEnergyPerTimeStep)*num_steps_per_img\n",
    "\n",
    "            if not measure_accuracy_runtime_trade_off:\n",
    "                snn_model.disconnect()\n",
    "\n",
    "    mean_vals_t1[:, k] = np.mean(vals_dense[0, :, k, :], 1)\n",
    "    std_vals_t1[:, k] = np.std(vals_dense[0, :, k, :], 1)\n",
    "\n",
    "    mean_vals_t2[:, k] = np.mean(vals_dense[3, :, k, :], 1)\n",
    "    std_vals_t2[:, k] = np.std(vals_dense[3, :, k, :], 1)\n",
    "\n",
    "    mean_vals_acc[:, k] = np.mean(vals_dense[1, :, k, :], 1)\n",
    "    std_vals_acc[:, k] = np.std(vals_dense[1, :, k, :], 1)\n",
    "\n",
    "    mean_vals_e1[:, k] = np.mean(vals_dense[2, :, k, :], 1)\n",
    "    std_vals_e1[:, k] = np.std(vals_dense[2, :, k, :], 1)\n",
    "\n",
    "    mean_vals_e2[:, k] = np.mean(vals_dense[4, :, k, :], 1)\n",
    "    std_vals_e2[:, k] = np.std(vals_dense[4, :, k, :], 1)\n",
    "\n",
    "total_list = [mean_vals_t1, std_vals_t1, mean_vals_t2, std_vals_t2, mean_vals_acc, \n",
    "              std_vals_acc, mean_vals_e1, std_vals_e1, mean_vals_e2, std_vals_e2]\n",
    "csv_names = [\"mean_vals_t1.csv\", \"std_vals_t1.csv\", \"mean_vals_t2.csv\", 'std_vals_t2.csv', 'mean_vals_acc.csv'\n",
    "            'std_vals_acc.csv', 'mean_vals_e1.csv', 'std_vals_e1.csv', 'mean_vals_e2.csv', 'std_vals_e2.csv']\n",
    "for i in range(len(csv_names)):\n",
    "    with open(csv_names[i], mode='w') as employee_file:\n",
    "        employee_writer = csv.writer(employee_file, delimiter=',',  quotechar='\"', \n",
    "                                     quoting=csv.QUOTE_MINIMAL)\n",
    "        employee_writer.writerow(total_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c79d54-c0e5-4888-bfe5-5af1f919b6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "for i in range(len(step_sizes)):\n",
    "    plt.errorbar(layer_sizes, mean_vals_t1[:, i], yerr = std_vals_t1[:, i],\n",
    "                 label = f\"step_sizes = {step_sizes[i]}\")\n",
    "    plt.ylabel(\"time [s]\")\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.title(\"Total Time over hidden layers\")\n",
    "    plt.xscale('log')\n",
    "    plt.legend()\n",
    "    \n",
    "\n",
    "fig.savefig('total_time_dense_layer_stepsize.jpg', bbox_inches='tight', dpi=150)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8134c245-0173-4d72-80df-a5f4baa248c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "for i in range(len(step_sizes)):\n",
    "    plt.errorbar(layer_sizes, mean_vals_e1[:, i], yerr = std_vals_e1[:, i],\n",
    "                label = f\"step_sizes = {step_sizes[i]}\")\n",
    "    plt.xlabel(\"Hidden Layer size\")\n",
    "    plt.ylabel(\"energy [µJ]\")\n",
    "    plt.title(\"Total Energy over hidden layers\")\n",
    "    plt.xscale('log')\n",
    "    plt.legend()\n",
    "    \n",
    "fig.savefig('total_energy_dense_layer_stepsize.jpg', bbox_inches='tight', dpi=150)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3189c389",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "for i in range(len(step_sizes)):\n",
    "    plt.errorbar(layer_sizes, mean_vals_acc[:, i], yerr = std_vals_acc[:, i],\n",
    "                label = f\"step_sizes = {step_sizes[i]}\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"Hidden Layer size\")\n",
    "    plt.title(\"Accuracy over hidden layers\")\n",
    "    plt.xscale('log')\n",
    "\n",
    "fig.savefig('accuracy_dense_layer_stepsize.jpg', bbox_inches='tight', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "01302fc1-a3f0-4ee3-9cc8-57f2537b421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "for i in range(len(step_sizes)):\n",
    "    plt.errorbar(layer_sizes, mean_vals_t2[:, i], yerr = std_vals_t2[:, i],\n",
    "                 label = f\"step_sizes = {step_sizes[i]}\")\n",
    "    plt.xlabel(\"Hidden Layer size\")\n",
    "    plt.ylabel(\"Time [ms]\")\n",
    "    plt.title(\"Time per time step over hidden layers\")\n",
    "    plt.xscale('log')\n",
    "    plt.legend()\n",
    "    \n",
    "\n",
    "fig.savefig('time_timestep_av_dense_layer_stepsize.jpg', bbox_inches='tight', dpi=150)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf92881-1a54-41a6-8012-e28fe0e32e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "for i in range(len(step_sizes)):\n",
    "    plt.errorbar(layer_sizes, mean_vals_e2[:, i], yerr = std_vals_e2[:, i],\n",
    "                label = f\"step_sizes = {step_sizes[i]}\")\n",
    "    plt.xlabel(\"Hidden Layer size\")\n",
    "    plt.ylabel(\"Energy [µJ]\")\n",
    "    plt.title(\"Energy per time step over hidden layers\")\n",
    "    plt.xscale('log')\n",
    "    plt.legend()\n",
    "    \n",
    "fig.savefig('energy_timestep_av_dense_layer_stepsize.jpg', bbox_inches='tight', dpi=150)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6f7671f-21d5-4d68-b042-715180e90f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_time_ann = np.mean(ann_time, axis = 1)\n",
    "sd_time_ann = np.std(ann_time, axis = 1)\n",
    "\n",
    "mean_acc_ann = np.mean(accuracy_ANN, axis = 1)\n",
    "sd_acc_ann = np.std(accuracy_ANN, axis = 1)\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.plot(layer_sizes, mean_time_ann)\n",
    "plt.errorbar(layer_sizes, mean_time_ann, yerr = sd_time_ann, fmt ='o', mfc = 'b', mec = 'b', ms = 2)\n",
    "plt.ylabel(\"time[s]\")\n",
    "plt.xlabel(\"Hidden Layer size\")\n",
    "plt.xscale('log')\n",
    "\n",
    "fig.savefig('ANN_time_dense.jpg', bbox_inches='tight', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "021d8db1-3cde-4a61-acdd-0dff014a15b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.plot(layer_sizes, mean_acc_ann)\n",
    "plt.errorbar(layer_sizes, mean_acc_ann, yerr = sd_acc_ann, fmt ='o', mfc = 'b', mec = 'b', ms = 2)\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"Hidden Layer size\")\n",
    "plt.xscale('log')\n",
    "\n",
    "fig.savefig('ANN_acc_dense.jpg', bbox_inches='tight', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d49ba3e-9233-4e05-a408-3af9dce8c4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Throughput values \n",
    "through_time = 1/(mean_vals_t1/num_test_samples)\n",
    "through_time_sd = 1/(std_vals_t1/(num_test_samples)**2)\n",
    "\n",
    "through_energy = (mean_vals_e1/num_test_samples)\n",
    "through_energy_sd = 1/(std_vals_e1/(num_test_samples)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f1ff2d-d61b-422c-85ac-b4ba6ef12ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "for i in range(len(step_sizes)):\n",
    "    plt.errorbar(layer_sizes, through_time[:, i], yerr = through_time_sd[:, i],\n",
    "                 label = f\"step_sizes = {step_sizes[i]}\")\n",
    "    plt.xlabel(\"Hidden Layer size\")\n",
    "    plt.ylabel(\"Inferences per Second\")\n",
    "    plt.title(\"Inference Time over hidden layer size\")\n",
    "    plt.xscale('log')\n",
    "    plt.legend()\n",
    "    \n",
    "\n",
    "fig.savefig('Inference_time_dense_layer_stepsize.jpg', bbox_inches='tight', dpi=150)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d55225d-d920-45a5-b00e-841e0fe08f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "for i in range(len(step_sizes)):\n",
    "    plt.errorbar(layer_sizes, through_energy[:, i], yerr = through_energy_sd[:, i],\n",
    "                label = f\"step_sizes = {step_sizes[i]}\")\n",
    "    plt.xlabel(\"Hidden Layer size\")\n",
    "    plt.ylabel(\"Energy per Inference [µJ]\")\n",
    "    plt.title(\"Inference Energy over hidden layer size\")\n",
    "    plt.xscale('log')\n",
    "    plt.legend()\n",
    "    \n",
    "fig.savefig('Inference_energy_dense_layer_stepsize.jpg', bbox_inches='tight', dpi=150)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e71a83-2d1f-44fd-bd2e-c557c5cc0a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "0c9fdf67ddc2e2392292f776e8d08f045c7d7e879e644d5d895c727cdf27b2ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-11 11:34:58.831893: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-11 11:34:59.083281: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /slurm/intel-archi/lib\n",
      "2023-01-11 11:34:59.083303: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-11 11:35:00.807298: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /slurm/intel-archi/lib\n",
      "2023-01-11 11:35:00.807370: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /slurm/intel-archi/lib\n",
      "2023-01-11 11:35:00.807377: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpacking file ../EME/SVDD/data/training.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event type int8 (3600000,) 3.6 MB\n",
      "regression float32 (3600000, 76) 1094.4 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " in_regression (InputLayer)  [(None, 76)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               39424     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " z_mean (Dense)              (None, 21)                2709      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 206,357\n",
      "Trainable params: 206,357\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-11 11:35:28.602463: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /slurm/intel-archi/lib\n",
      "2023-01-11 11:35:28.604685: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-11 11:35:28.604700: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ncl-edu): /proc/driver/nvidia/version does not exist\n",
      "2023-01-11 11:35:28.604918: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "# %matplotlib inline\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dropout, Flatten, Conv2D, Input\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,RobustScaler \n",
    "\n",
    "sys.path.append(\"/homes/lexjohan/Documents/EME/SVDD/\")\n",
    "from dataloader import unpack, unpack_ordered, unpack_polina\n",
    "from modeldefault import VariationalAutoencoderModel\n",
    "\n",
    "sys.path.append('/homes/lexjohan/Documents/models')\n",
    "os.path.join(\"nxsdk_modules_ncl/snntoolbox\")\n",
    "from nxsdk_modules_ncl.dnn.src.utils import extract, to_integer\n",
    "from nxsdk_modules_ncl.dnn.src.dnn_layers import NxInputLayer, NxDense, \\\n",
    "    NxModel, ProbableStates\n",
    "from nxsdk_modules_ncl.dnn.composable.composable_dnn import ComposableDNN as DNN\n",
    "from nxsdk_modules_ncl.input_generator.input_generator import InputGenerator\n",
    "import nxsdk\n",
    "from nxsdk.composable.model import Model\n",
    "from nxsdk.logutils.nxlogging import set_verbosity,LoggingLevel\n",
    "from nxsdk.api.enums.api_enums import ProbeParameter\n",
    "from nxsdk.graph.monitor.probes import PerformanceProbeCondition\n",
    "\n",
    "# Enable SLURM to run network on Loihi.\n",
    "os.environ['SLURM'] = '1'\n",
    "os.environ['PYTHONUNBUFFERED'] = '1'\n",
    "\n",
    "os.environ[\"PARTITION\"] = \"nahuku32_2h\"\n",
    "os.environ[\"BOARD\"] = 'ncl-ext-ghrd-01'\n",
    "\n",
    "mode = 'ordered' #mode = 'polina' / mode = 'None'\n",
    "Flags = None\n",
    "\n",
    "data_path = \"../EME/SVDD/data/\"\n",
    "\n",
    "train_path = data_path + \"training.h5\"\n",
    "test_path = data_path + \"testing.h5\"\n",
    "\n",
    "if mode == 'ordered':\n",
    "    _, regression = unpack_ordered(train_path, Flags)\n",
    "elif mode == 'polina':\n",
    "    _, num_objects, regression = unpack_polina(train_path, Flags)\n",
    "else:\n",
    "    #load training and validation data\n",
    "    _, num_objects, regression, classification = unpack(train_path, Flags)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(regression)\n",
    "reg_normalized = scaler.transform(regression)\n",
    "\n",
    "if mode == 'ordered':\n",
    "    training_data = reg_normalized[:]\n",
    "elif mode == 'polina':\n",
    "    training_data = [num_objects[:], reg_normalized[:]]\n",
    "else:\n",
    "    training_data = [classification[:], reg_normalized[:]]\n",
    "\n",
    "dataset_len = regression.shape[0]\n",
    "if mode == 'ordered' or mode == 'polina':\n",
    "    data_dim = regression.shape[1]\n",
    "else:\n",
    "    data_dim = classification.shape[1]\n",
    "\n",
    "models_path = \"../EME/SVDD/optimized_W/\"\n",
    "\n",
    "models = os.listdir(models_path)\n",
    "model_name = models[0]\n",
    "dim_z = int(model_name.split(\"zdim\")[1].split(\".\")[0].split(\"_\")[1])\n",
    "ft = 0\n",
    "\n",
    "hidden_layers = [512, 256, 128]\n",
    "\n",
    "ann_model = VariationalAutoencoderModel(hidden_layers, model_name, data_dim, dataset_len, dim_z, ft, mode=mode, verbose=True)\n",
    "ann_model.load_weights(models_path + model_name)\n",
    "\n",
    "def run_model(num_steps_per_sample, train_data):\n",
    "    batch_size = 64\n",
    "    num_samples = train_data.shape[0]\n",
    "    print(\"num_samples: {}\".format(num_samples))\n",
    "    print(f\"time steps: {num_steps_per_sample * num_samples}\")\n",
    "\n",
    "    print(\"starting model run\")\n",
    "    tStart = time.time()\n",
    "    snn_model.run(num_steps_per_sample * num_samples, aSync=True)\n",
    "    tEndBoot = time.time()\n",
    "\n",
    "    print(\"queueing data in input generator\")\n",
    "    with tqdm(total=num_samples) as pbar:\n",
    "        for idx in range(0, num_samples, batch_size):\n",
    "            batch = train_data[idx:idx+batch_size]\n",
    "            b = np.abs(np.rint(batch).astype(int))\n",
    "            input_generator.batchEncode(b)\n",
    "            pbar.update(batch_size)\n",
    "\n",
    "    tEndInput = time.time()\n",
    "    print(\"Reading out channels\")\n",
    "\n",
    "    out = list(dnn.readout_channel.read(num_samples))\n",
    "    tEndClassification = time.time()\n",
    "\n",
    "    print(\"Finishing model run\")\n",
    "    snn_model.finishRun()\n",
    "    \n",
    "    return tStart, tEndBoot, tEndInput, tEndClassification, out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "\n",
      "x\n",
      "\n",
      "..x\n",
      "\n",
      "x\n",
      "\n",
      "x\n",
      "\n",
      "num_samples: 36000s... sters... \n",
      "time steps: 900000\n",
      "starting model run\n",
      "queueing data in input generator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36032it [00:44, 806.06it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading out channels\n",
      "Finishing model run\n",
      "x   Processing timeseries... \n",
      "\n",
      "x\n",
      "\n",
      "..x\n",
      "\n",
      "x\n",
      "\n",
      "x\n",
      "\n",
      "num_samples: 36000s... sters... \n",
      "time steps: 900000\n",
      "starting model run\n",
      "queueing data in input generator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36032it [00:44, 806.37it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading out channels\n",
      "Finishing model run\n",
      "    Processing timeseries... \r"
     ]
    }
   ],
   "source": [
    "num_steps = 2\n",
    "\n",
    "vth_mant = 2**9\n",
    "bias_exp = 6\n",
    "weight_exponent = 0\n",
    "synapse_encoding = 'sparse'\n",
    "\n",
    "in_regression = NxInputLayer(data_dim,\n",
    "                            vThMant=vth_mant,\n",
    "                            biasExp=bias_exp)\n",
    "\n",
    "layer = NxDense(hidden_layers[0])(in_regression.input)\n",
    "for idx in range(1, len(hidden_layers)):\n",
    "    layer = NxDense(hidden_layers[idx])(layer)\n",
    "\n",
    "layer = NxDense(dim_z)(layer)\n",
    "\n",
    "set_verbosity(LoggingLevel.ERROR)\n",
    "# Extract weights and biases from parameter list.\n",
    "parameters = ann_model.model.get_weights()\n",
    "weights = parameters[0::2]\n",
    "biases = parameters[1::2]\n",
    "\n",
    "# Quantize weights and biases using max-normalization (Strong quantization loss if distributions have large tails)\n",
    "parameters_int = []\n",
    "for w, b in zip(weights, biases):\n",
    "    w_int, b_int = to_integer(w, b, 8)\n",
    "    parameters_int += [w_int, b_int]\n",
    "\n",
    "num_steps_per_img = 25\n",
    "data_dim_snn = (data_dim, )\n",
    "\n",
    "data_split = 0.01\n",
    "num_test_samples = int(training_data.shape[0] * data_split)\n",
    "execution_time_probe_bin_size = 512 # Small numbers slows down execution\n",
    "\n",
    "for n in range(num_steps):\n",
    "\n",
    "    snn_nxmodel = NxModel(in_regression.input, layer)\n",
    "    # Set quantized weigths and biases for spiking model\n",
    "    snn_nxmodel.set_weights(parameters_int)\n",
    "\n",
    "    dnn = DNN(model=snn_nxmodel, num_steps_per_img=num_steps_per_img)\n",
    "\n",
    "    input_generator = InputGenerator(shape=data_dim_snn, interval=num_steps_per_img)\n",
    "    input_generator.setBiasExp(bias_exp)\n",
    "\n",
    "    snn_model = nxsdk.composable.model.Model(\"dnn_model\")\n",
    "    snn_model.add(dnn)\n",
    "    snn_model.add(input_generator)\n",
    "\n",
    "    input_generator.connect(dnn)\n",
    "\n",
    "    input_generator.processes.inputEncoder.executeAfter(dnn.processes.reset)\n",
    "    snn_model.compile()\n",
    "\n",
    "    eProbe = snn_model.board.probe(\n",
    "        probeType=ProbeParameter.ENERGY, \n",
    "        probeCondition=PerformanceProbeCondition(\n",
    "            tStart=1, \n",
    "            tEnd=num_test_samples*num_steps_per_img, \n",
    "            bufferSize=1024, \n",
    "            binSize=execution_time_probe_bin_size))\n",
    "\n",
    "    snn_model.start(snn_nxmodel.board)\n",
    "    tStart, tEndBoot, tEndInput, tEndClassification, out = run_model(num_steps_per_img, training_data[:num_test_samples])\n",
    "    snn_model.disconnect()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  5 2022, 06:56:58) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0c9fdf67ddc2e2392292f776e8d08f045c7d7e879e644d5d895c727cdf27b2ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

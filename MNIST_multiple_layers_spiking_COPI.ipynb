{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bc3164b-a8e4-4f13-8ca6-1e6b46abf02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Add path to the nxsdk_modules package.\n",
    "sys.path.append('/homes/dcicch/models')\n",
    "sys.path.append('/homes/dcicch/NxSDK_Package/nxsdk-apps')\n",
    "\n",
    "\n",
    "# Enable SLURM to run network on Loihi.\n",
    "os.environ['SLURM'] = '1'\n",
    "os.environ['PYTHONUNBUFFERED'] = '1'\n",
    "\n",
    "os.environ[\"PARTITION\"] = \"nahuku32_2h\"\n",
    "os.environ[\"BOARD\"] = 'ncl-ext-ghrd-01'\n",
    "\n",
    "snipDir = os.path.abspath(os.path.join('..', 'snips', 'reset_model_states'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7c67b8e-972a-4c99-bc13-ba30c9da4aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "#from keras.models import Model\n",
    "from keras.layers import Dropout, Flatten, Conv2D, Input, Dense\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from abc import ABC\n",
    "from collections import namedtuple\n",
    "\n",
    "from nxsdk_modules_ncl.dnn.src.utils import extract\n",
    "from nxsdk_modules_ncl.dnn.src.dnn_layers import NxInputLayer, NxConv2D, NxDense, \\\n",
    "    NxModel, NxFlatten, ProbableStates\n",
    "\n",
    "from nxsdk_modules_ncl.dnn.src.optimization import ExclusionCriteria\n",
    "from nxsdk_modules_ncl.dnn.composable.composable_dnn import ComposableDNN\n",
    "\n",
    "from nxsdk_modules_ncl.dnn.src.utils import to_integer\n",
    "\n",
    "from nxsdk_modules_ncl.dnn.composable.composable_dnn import ComposableDNN as DNN\n",
    "from nxsdk_modules.input_generator.input_generator import InputGenerator\n",
    "\n",
    "from nxsdk.composable.model import Model\n",
    "from nxsdk.logutils.nxlogging import set_verbosity,LoggingLevel\n",
    "import csv\n",
    "#from nxsdk.api.n2a as \n",
    "set_verbosity(LoggingLevel.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5c6cdd3-90ba-4653-9b54-f713ea94feb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_training_epochs = 2\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# set later\n",
    "#num_steps_per_img = 75\n",
    "num_train_samples = 60000\n",
    "num_test_samples = 128\n",
    "\n",
    "# EnergyProbes allow to profile execution time, power and thus energy consumption\n",
    "enable_energy_probe = True\n",
    "# set later\n",
    "#execution_time_probe_bin_size = 75 # Small numbers slows down execution\n",
    "\n",
    "# Not yet supported\n",
    "measure_accuracy_runtime_trade_off = False # Not yet supported\n",
    "runtimes = [128, 256, 512, 1024, 2048]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3479f64c-9bfc-4b47-a1b4-f4469c4d92d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Load standard MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize input so we can train ANN with it. \n",
    "# Will be converted back to integers for SNN layer.\n",
    "x_train = x_train[:num_train_samples, :, :] / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# Add a channel dimension.\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "# One-hot encode target vectors.\n",
    "y_train = np_utils.to_categorical(y_train[:num_train_samples], 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f972b2f0-fb36-4082-81be-f2f4b6602977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModel(num_steps_per_sample, x_test, y_test):\n",
    "    \"\"\"Runs the SNN Model to classify test images.\"\"\"\n",
    "      \n",
    "    # Initialize arrays for results\n",
    "    num_samples = len(y_test)\n",
    "    classifications = np.zeros(num_samples, int)\n",
    "    labels = np.zeros(num_samples, int)\n",
    "\n",
    "    # Run DNN to classify images\n",
    "    tStart = time.time()\n",
    "    snn_model.run(num_steps_per_sample * num_samples, aSync=True)\n",
    "    tEndBoot = time.time()\n",
    "\n",
    "    # Enqueue images by pushing them into InputGenerator\n",
    "    print(\"Queuing images...\")\n",
    "    for i, (x, y) in enumerate(zip((x_test* 255).astype(int), y_test)):\n",
    "        input_generator.encode(x)\n",
    "        labels[i] = np.argmax(y)\n",
    "    tEndInput = time.time()\n",
    "\n",
    "    # Read out classification results for all images\n",
    "    print(\"Waiting for classification to finish...\")\n",
    "    classifications = list(dnn.readout_channel.read(num_samples))\n",
    "    tEndClassification = time.time()\n",
    "    \n",
    "    # finishRun fetches EnergyProbe data if configured\n",
    "    snn_model.finishRun()\n",
    "    \n",
    "    return tStart, tEndBoot, tEndInput, tEndClassification, classifications, labels\n",
    "\n",
    "def calcAccuracy(classifications, labels):\n",
    "    \"\"\"Computes classification accuracy for a set of images given classification and labels.\"\"\"\n",
    "    errors = classifications != labels\n",
    "    num_errors = np.sum(errors)\n",
    "    num_samples = len(classifications)\n",
    "    return (num_samples-num_errors)/num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f207348e-8840-4551-b034-be55528b5ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(samples, guesses, lables, numCols=5, size=28):\n",
    "    \"\"\"Plots samples as an array of images.\"\"\"\n",
    "    \n",
    "    import math\n",
    "    numSamples = len(samples)\n",
    "    numRows = int(math.ceil(numSamples/numCols))\n",
    "    plt.figure(3, figsize=(20, 10))\n",
    "    i = 0\n",
    "    for c in range(numCols):\n",
    "        for r in range(numRows):\n",
    "            plt.subplot(numRows, numCols, i+1)\n",
    "            plt.imshow(np.reshape(samples[i,:], (size, size)))\n",
    "            plt.axis('off')\n",
    "            plt.title('C:{}/L:{}'.format(guesses[i], lables[i]))\n",
    "            i += 1\n",
    "            if i == numSamples:\n",
    "                break\n",
    "        if i == numSamples:\n",
    "            break\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7de7e74-3fc7-4c5a-905f-9c5eaf3486eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def batch_data(data, batch_size):\n",
    "    data_size, n_features = data.shape\n",
    "    n_batches = int(data_size / batch_size)\n",
    "    batched_data = np.zeros((n_batches, batch_size, n_features))\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        start = batch_size * i\n",
    "        batched_data[i] = data[start: start + batch_size]\n",
    "    return batched_data\n",
    "\n",
    "\n",
    "def derivative_relu(x, alpha=0.1):\n",
    "    return tf.where(x < 0, alpha, 1.0)\n",
    "\n",
    "def copi_backpropagation(**kwargs):\n",
    "    \"\"\" returns all deltas for dloss/da_l \"\"\"\n",
    "    df = kwargs['df']\n",
    "    acts = kwargs['acts']\n",
    "    ys = kwargs['ys']\n",
    "    y_hat = kwargs['y_hat']\n",
    "    alpha = kwargs['alpha_relu']\n",
    "    layers = kwargs['layers']\n",
    "\n",
    "    d = -df(acts[-1], alpha) * (ys[-1] - y_hat)\n",
    "    deltas = [d]\n",
    "    for i in reversed(range(0, len(layers) - 1)):\n",
    "        W = layers[i + 1].W\n",
    "        R = layers[i + 1].R\n",
    "\n",
    "        d = (d @ tf.transpose(W) @ tf.transpose(R)) * df(acts[i], alpha)\n",
    "        deltas.append(d)\n",
    "    return reversed(deltas)\n",
    "\n",
    "\n",
    "def copi_broadcast_alignment(**kwargs):\n",
    "    \"\"\" returns all deltas for dloss/da_l using a random feedback matrix B \"\"\"\n",
    "    ys = kwargs['ys']\n",
    "    y_hat = kwargs['y_hat']\n",
    "    layers = kwargs['layers']\n",
    "\n",
    "    error = y_hat - ys[-1]\n",
    "\n",
    "    deltas = []\n",
    "    for layer in layers[:-1]:\n",
    "        feedback = error @ layer.B\n",
    "        deltas.append(feedback)\n",
    "    deltas.append(error)\n",
    "\n",
    "    return deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "239f433a-351b-44de-91d8-59d0ebd5a2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer\n",
    "InternalState = namedtuple('InternalState', ('u', 'r', 'v'))\n",
    "\n",
    "\n",
    "class LIF(ABC, keras.layers.Layer):\n",
    "    def __init__(self, n_neurons: int, n_err: int, rest: float = 0.0, threshold: float = 0.4, refractory: float = 1.0,\n",
    "                 tau: float = 20.0, dt: float = 0.25, epsilon: float = 0.001,\n",
    "                 train_fn_name=None, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.n_neurons = n_neurons\n",
    "        self.resting_potential = tf.cast(rest, self.dtype)\n",
    "        self.threshold = tf.cast(threshold, self.dtype)\n",
    "        self.refractory = tf.cast(refractory, self.dtype)\n",
    "        self.tau = tf.cast(tau, self.dtype)\n",
    "        self.eps = tf.cast(epsilon, self.dtype)\n",
    "        self.dt = tf.cast(dt, self.dtype)\n",
    "\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        if train_fn_name == 'ba':\n",
    "            self.B = self.add_weight('B', shape=(n_err, n_neurons), initializer='GlorotUniform', trainable=False)\n",
    "        self.state = None\n",
    "        self.inputs = None\n",
    "        self.alpha = tf.exp(-self.dt / self.tau)\n",
    "\n",
    "    def build(self, input_shape: list) -> None:\n",
    "        batch_size, n_inputs = input_shape\n",
    "        self.inputs = tf.zeros(n_inputs, dtype=self.dtype)\n",
    "\n",
    "        self.W = self.add_weight('W', shape=(n_inputs, self.n_neurons), initializer='GlorotUniform', trainable=True)\n",
    "        self.b = self.add_weight('b', shape=(self.n_neurons,), initializer='GlorotUniform', trainable=True)\n",
    "        self.state = self.zero_state(batch_size)\n",
    "\n",
    "    def zero_state(self, batch_size: int) -> InternalState:\n",
    "        return InternalState(\n",
    "            u=tf.fill((batch_size, self.n_neurons), tf.cast(self.resting_potential, dtype=self.dtype)),\n",
    "            r=tf.zeros((batch_size, self.n_neurons), self.dtype),\n",
    "            v=tf.zeros((batch_size, self.n_neurons), self.dtype)\n",
    "        )\n",
    "\n",
    "    def call(self, inputs: tf.Tensor, *args, **kwargs) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        simulate one time-step, updates internal values and returns whether a spike occurred or not\n",
    "\n",
    "        :param inputs: one dimensional tensor of inputs with size n_in\n",
    "        :return: boolean tensor of spike or non-spike values\n",
    "        \"\"\"\n",
    "\n",
    "        # old internal state\n",
    "        u, r, _ = self.state\n",
    "\n",
    "        v = inputs @ self.W + self.b\n",
    "        new_r = tf.where(r > 0., r + self.dt, r)\n",
    "        new_r = tf.where(new_r > self.refractory, 0., new_r)\n",
    "        new_u = tf.where(new_r > 0., self.resting_potential, u + (v - u) * (self.dt / self.tau))\n",
    "        new_r = tf.where(new_u > self.threshold, self.eps, new_r)\n",
    "\n",
    "        # new observable state\n",
    "        z = tf.cast(tf.where(new_r > 0, 1.0, 0.0), self.dtype)\n",
    "\n",
    "        # new internal state\n",
    "        self.state = InternalState(new_u, new_r, v)\n",
    "        self.inputs = inputs\n",
    "\n",
    "        return z\n",
    "\n",
    "    def reset(self, batch_size: int):\n",
    "        self.state = self.zero_state(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a8fcb24-806e-4363-832a-78d3309d6af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# network\n",
    "class CopiLayer(keras.layers.Layer):\n",
    "    def __init__(self, units, activation, n_err=None, *args, **kwargs):\n",
    "        super(CopiLayer, self).__init__(*args, **kwargs)\n",
    "        self.n = units\n",
    "        self.activation = activation\n",
    "        if n_err is not None:\n",
    "            self.B = self.add_weight(name='W', shape=(n_err, self.n),\n",
    "                                     initializer='GlorotNormal',\n",
    "                                     trainable=False)\n",
    "\n",
    "        self.W = None\n",
    "        self.R = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        _, n_in = input_shape\n",
    "\n",
    "        self.W = self.add_weight(name='W', shape=(n_in, self.n), initializer='GlorotNormal', trainable=True)\n",
    "        self.R = self.add_weight(name='R', shape=(n_in, n_in), initializer='Identity', trainable=True)\n",
    "\n",
    "    def call(self, y_prev, *args, **kwargs):\n",
    "        x = y_prev @ self.R\n",
    "        a = x @ self.W\n",
    "        y = self.activation(a)\n",
    "        return x, a, y\n",
    "\n",
    "\n",
    "class CopiNetwork(keras.Model):\n",
    "    def __init__(self, units, activations, lr_w, lr_r, df, alpha, rule='bp', *args, **kwargs):\n",
    "        super(CopiNetwork, self).__init__(*args, **kwargs)\n",
    "        n_err = None\n",
    "        if rule == 'bp':\n",
    "            self.rule = copi_backpropagation\n",
    "        elif rule == 'ba':\n",
    "            self.rule = copi_broadcast_alignment\n",
    "            n_err = units[-1]\n",
    "        else:\n",
    "            print(f\"rule not supported yet; choose between 'ba' and 'bp'\")\n",
    "\n",
    "        self.ls = [CopiLayer(u, a, n_err) for u, a in zip(units, activations)]\n",
    "        self.df = df\n",
    "        self.alpha = alpha\n",
    "        self.lr_w = lr_w\n",
    "        self.lr_r = lr_r\n",
    "        if rule == 'bp':\n",
    "            self.rule = copi_backpropagation\n",
    "        elif rule == 'ba':\n",
    "            self.rule = copi_broadcast_alignment\n",
    "        else:\n",
    "            print(f\"rule not supported yet; choose between 'ba' and 'bp'\")\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None, **kwargs):\n",
    "        xs, acts, ys = [], [], []\n",
    "\n",
    "        for layer in self.ls:\n",
    "            x, a, y = layer(inputs)\n",
    "            inputs = y\n",
    "            xs.append(x)\n",
    "            acts.append(a)\n",
    "            ys.append(y)\n",
    "\n",
    "        return xs, acts, ys if training else ys\n",
    "\n",
    "    def train_step(self, data, *args, **kwargs):\n",
    "        inputs, y_true = data\n",
    "        batch_size, feature_size = inputs.shape\n",
    "\n",
    "        # forward pass\n",
    "        xs, acts, ys = self(inputs, True)\n",
    "\n",
    "        # backward pass\n",
    "        rule_info_dict = {\n",
    "            'df': self.df,\n",
    "            'acts': acts,\n",
    "            'ys': ys,\n",
    "            'y_hat': y_true,\n",
    "            'alpha_relu': self.alpha,\n",
    "            'layers': self.ls\n",
    "        }\n",
    "\n",
    "        deltas = self.rule(**rule_info_dict)\n",
    "\n",
    "        # mixed update pass\n",
    "        for x, a, delta, layer in zip(xs, acts, deltas, self.ls):\n",
    "            W, R = layer.trainable_variables\n",
    "\n",
    "            z = a + delta\n",
    "            q = x @ R\n",
    "            xT = tf.transpose(x)\n",
    "            squares = (xT @ x) * tf.eye(x.shape[-1])  # diag(E[x^2])\n",
    "\n",
    "            dW = (xT @ z - squares @ W) / batch_size\n",
    "            dR = (xT @ q - squares @ R) / batch_size\n",
    "\n",
    "            layer.W.assign_add(self.lr_w * dW)\n",
    "            layer.R.assign_sub(self.lr_r * dR)\n",
    "\n",
    "        self.compiled_metrics.update_state(y_true, ys[-1])\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        ys = self(x)[-1]\n",
    "\n",
    "        self.compiled_metrics.update_state(y, ys[-1])\n",
    "        return {m.name: m.result() for m in self.metrics}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6920b656-3efc-43bc-9219-d8b14a4b612c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of time steps: 10\n",
      "Neurons in hidden layer: 10\n",
      "Epoch 1/2\n",
      "60/60 [==============================] - 115s 2s/step - mse: 0.0372 - accuracy: 0.7470\n",
      "Epoch 2/2\n",
      "60/60 [==============================] - 114s 2s/step - mse: 0.0219 - accuracy: 0.8778\n",
      "313/313 [==============================] - 62s 197ms/step - mse: 0.0217 - accuracy: 0.8821\n",
      "evaluation mse, accuracy [0.021712832152843475, 0.882099986076355]\n",
      "x\n",
      "\n",
      "x\n",
      "\n",
      "x\n",
      "\n",
      "x\n",
      "\n",
      "<nxsdk.driver.host_coordinator.HostCoordinator object at 0x7efc3468a190>\n",
      "Queuing images...spikes... . .. \n",
      "Waiting for classification to finish...\n",
      "Epoch 1/2ssing timeseries... \n",
      " 9/60 [===>..........................] - ETA: 1:43 - mse: 0.0666 - accuracy: 0.4676"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [33], line 83\u001b[0m\n\u001b[1;32m     78\u001b[0m     net \u001b[38;5;241m=\u001b[39m LIFNetwork(layers,\n\u001b[1;32m     79\u001b[0m                      nt\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, burn_in\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, grad_fn_name\u001b[38;5;241m=\u001b[39mgrad_fn_name, surrogate_fn\u001b[38;5;241m=\u001b[39mfn, gen\u001b[38;5;241m=\u001b[39mpoisson_gen,\n\u001b[1;32m     80\u001b[0m                      dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     81\u001b[0m     net\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m], run_eagerly\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 83\u001b[0m     net\u001b[38;5;241m.\u001b[39mfit(train_X, train_y, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[training_logger])\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevaluation mse, accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, net\u001b[38;5;241m.\u001b[39mevaluate(test_X, test_y))\n\u001b[1;32m     87\u001b[0m vth_mant \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m9\u001b[39m\n",
      "File \u001b[0;32m~/nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages/keras/engine/training.py:1249\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_function\u001b[39m(iterator):\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;124;03m\"\"\"Runs a training execution with a single step.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstep_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages/keras/engine/training.py:1233\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m   1229\u001b[0m     run_step \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mfunction(\n\u001b[1;32m   1230\u001b[0m         run_step, jit_compile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, reduce_retracing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1231\u001b[0m     )\n\u001b[1;32m   1232\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iterator)\n\u001b[0;32m-> 1233\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1234\u001b[0m outputs \u001b[38;5;241m=\u001b[39m reduce_per_replica(\n\u001b[1;32m   1235\u001b[0m     outputs,\n\u001b[1;32m   1236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy,\n\u001b[1;32m   1237\u001b[0m     reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_reduction_method,\n\u001b[1;32m   1238\u001b[0m )\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1316\u001b[0m, in \u001b[0;36mStrategyBase.run\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscope():\n\u001b[1;32m   1312\u001b[0m   \u001b[38;5;66;03m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[39;00m\n\u001b[1;32m   1313\u001b[0m   \u001b[38;5;66;03m# applied when the caller is also in Eager mode.\u001b[39;00m\n\u001b[1;32m   1314\u001b[0m   fn \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[1;32m   1315\u001b[0m       fn, autograph_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 1316\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extended\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_for_each_replica\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2895\u001b[0m, in \u001b[0;36mStrategyExtendedV1.call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2893\u001b[0m   kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   2894\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy()\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m-> 2895\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_for_each_replica\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3696\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3694\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_for_each_replica\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs):\n\u001b[1;32m   3695\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ReplicaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy(), replica_id_in_sync_group\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m-> 3696\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:595\u001b[0m, in \u001b[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    594\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mControlStatusCtx(status\u001b[38;5;241m=\u001b[39mag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mUNSPECIFIED):\n\u001b[0;32m--> 595\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages/keras/engine/training.py:1222\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_step\u001b[39m(data):\n\u001b[0;32m-> 1222\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1223\u001b[0m     \u001b[38;5;66;03m# Ensure counter is updated only if `train_step` succeeds.\u001b[39;00m\n\u001b[1;32m   1224\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "Cell \u001b[0;32mIn [16], line 26\u001b[0m, in \u001b[0;36mLIFNetwork.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     24\u001b[0m x, y \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     25\u001b[0m y \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m---> 26\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls:\n\u001b[1;32m     29\u001b[0m     layer\u001b[38;5;241m.\u001b[39mreset(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[0;32mIn [14], line 5\u001b[0m, in \u001b[0;36mpoisson_gen\u001b[0;34m(x, nt, dtype)\u001b[0m\n\u001b[1;32m      3\u001b[0m batch_size, features \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m      4\u001b[0m res \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform((batch_size, features, nt))\n\u001b[0;32m----> 5\u001b[0m br_x \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_to\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcast(tf\u001b[38;5;241m.\u001b[39mwhere(res \u001b[38;5;241m<\u001b[39m br_x, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), dtype)\n",
      "File \u001b[0;32m~/nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py:863\u001b[0m, in \u001b[0;36mbroadcast_to\u001b[0;34m(input, shape, name)\u001b[0m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m _result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m--> 863\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbroadcast_to_eager_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_SymbolicException:\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
      "File \u001b[0;32m~/nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py:909\u001b[0m, in \u001b[0;36mbroadcast_to_eager_fallback\u001b[0;34m(input, shape, name, ctx)\u001b[0m\n\u001b[1;32m    907\u001b[0m _inputs_flat \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28minput\u001b[39m, shape]\n\u001b[1;32m    908\u001b[0m _attrs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, _attr_T, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTidx\u001b[39m\u001b[38;5;124m\"\u001b[39m, _attr_Tidx)\n\u001b[0;32m--> 909\u001b[0m _result \u001b[38;5;241m=\u001b[39m \u001b[43m_execute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBroadcastTo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_inputs_flat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_attrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _execute\u001b[38;5;241m.\u001b[39mmust_record_gradient():\n\u001b[1;32m    912\u001b[0m   _execute\u001b[38;5;241m.\u001b[39mrecord_gradient(\n\u001b[1;32m    913\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBroadcastTo\u001b[39m\u001b[38;5;124m\"\u001b[39m, _inputs_flat, _attrs, _result)\n",
      "File \u001b[0;32m~/nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## timing ANN\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "class TimingCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, logs={}):\n",
    "        self.logs=[]\n",
    "    def on_test_begin(self, logs={}):\n",
    "        self.starttime = timer()\n",
    "    def on_test_end(self, logs={}):\n",
    "        self.logs.append(timer()-self.starttime)\n",
    "\n",
    "train_model = True\n",
    "\n",
    "(train_X, train_y), (test_X, test_y) = tf.keras.datasets.mnist.load_data(\n",
    "    path='mnist.npz')\n",
    "train_X = train_X.reshape(-1, 784) / 255\n",
    "train_y = tf.one_hot(train_y, depth=10, axis=1)\n",
    "test_X = test_X.reshape(-1, 784) / 255\n",
    "test_y = tf.one_hot(test_y, depth=10, axis=1)\n",
    "     \n",
    "## Array for time\n",
    "## Array for accuracy\n",
    "## Array for Energy\n",
    "vals_dense = np.zeros((5, 5, 10))\n",
    "    \n",
    "    \n",
    "# Path for pre-trained model\n",
    "pretrained_model_path = os.path.join(os.path.abspath(''),\n",
    "                                     'models', \n",
    "                                     'a_minist_model.h5')\n",
    "\n",
    "layer_sizes = [10, 40, 100, 400, 1000]\n",
    "step_sizes = [30, 50, 100]\n",
    "\n",
    "ann_time = np.zeros((len(layer_sizes), 10))\n",
    "accuracy_ANN = np.zeros((len(layer_sizes), 10))\n",
    "vals_dense = np.zeros((5, len(layer_sizes), len(step_sizes), 10))\n",
    "\n",
    "mean_vals_t1 = np.zeros((len(layer_sizes), len(step_sizes)))\n",
    "std_vals_t1 = np.zeros((len(layer_sizes), len(step_sizes)))\n",
    "mean_vals_t2 = np.zeros((len(layer_sizes), len(step_sizes)))\n",
    "std_vals_t2 = np.zeros((len(layer_sizes), len(step_sizes)))\n",
    "mean_vals_acc = np.zeros((len(layer_sizes), len(step_sizes)))\n",
    "std_vals_acc = np.zeros((len(layer_sizes), len(step_sizes)))\n",
    "mean_vals_e1 = np.zeros((len(layer_sizes), len(step_sizes)))\n",
    "std_vals_e1 = np.zeros((len(layer_sizes), len(step_sizes)))\n",
    "mean_vals_e2 = np.zeros((len(layer_sizes), len(step_sizes)))\n",
    "std_vals_e2 = np.zeros((len(layer_sizes), len(step_sizes)))\n",
    "\n",
    "\n",
    "n_dim = 10\n",
    "    \n",
    "# Generate model\n",
    "for k in range(len(step_sizes)):\n",
    "    print(f\"Number of time steps: {step_sizes[k]}\")\n",
    "    num_steps_per_img = step_sizes[k]\n",
    "    execution_time_probe_bin_size = step_sizes[k]\n",
    "    for i in range(len(layer_sizes)):\n",
    "        print(f\"Neurons in hidden layer: {layer_sizes[i]}\")\n",
    "        for j in range(10):\n",
    "            if train_model or not os.path.isfile(pretrained_model_path):\n",
    "                # set all variable accordingly\n",
    "#                 num_steps_per_img = 100  # timesteps per image\n",
    "                layers = [layer_sizes[i], 10]  # neurons per layer\n",
    "                units = [layer_size, 10]\n",
    "\n",
    "#                 training_logger = keras.callbacks.CSVLogger('train_log.csv', separator=\",\", append=True)\n",
    "                net = CopiNetwork(units, activations,\n",
    "                              lr_w=1e-2, lr_r=1e-2,\n",
    "                              df=derivative_relu,\n",
    "                              alpha=alpha,\n",
    "                              rule='bp')\n",
    "                net.compile(optimizer='adam', loss='mse', metrics=['categorical_accuracy', 'mse'])\n",
    "                net.load_weights('weights/weights-time_step{}-layer_size{}-n{}-sg'.format(num_steps_per_img, layer_sizes[i], n))\n",
    "\n",
    "#                 net.fit(train_X, train_y, epochs=10, batch_size=1000, callbacks=[training_logger])\n",
    "#                 print('evaluation mse, accuracy', net.evaluate(test_X, test_y))\n",
    "\n",
    "\n",
    "            vth_mant = 2**9\n",
    "            bias_exp = 6\n",
    "            weight_exponent = 0\n",
    "            synapse_encoding = 'sparse'\n",
    "            #exclusionCriteria = ExclusionCriteria()\n",
    "\n",
    "            inputLayer = NxInputLayer((28,28), \n",
    "                                         vThMant=vth_mant, \n",
    "                                         biasExp=bias_exp)\n",
    "\n",
    "            input_layer = NxFlatten()(inputLayer.input)\n",
    "\n",
    "            layer = NxDense(layer_sizes[i], activation='relu')(input_layer)\n",
    "\n",
    "            outputs = NxDense(10, activation='softmax')(layer)\n",
    "\n",
    "            snn_nxmodel = NxModel(inputLayer.input, outputs, numCandidatesToCompute=1)\n",
    "\n",
    "            #snn_nxmodel.summary()\n",
    "\n",
    "\n",
    "            # Extract weights and biases from parameter list.\n",
    "            parameters = net.get_weights()\n",
    "            weights = parameters[0::2]\n",
    "            biases = parameters[1::2]\n",
    "\n",
    "            # Quantize weights and biases using max-normalization (Strong quantization loss if distributions have large tails)\n",
    "            parameters_int = []\n",
    "            for w, b in zip(weights, biases):\n",
    "                w_int, b_int = to_integer(w, b, 8)\n",
    "                parameters_int += [w_int, b_int]\n",
    "\n",
    "            # Set quantized weigths and biases for spiking model\n",
    "            snn_nxmodel.set_weights(parameters_int)\n",
    "\n",
    "\n",
    "            # NxModel is not yet implemented as a Composable -> Wrap it with DNN composable class\n",
    "            dnn = DNN(model=snn_nxmodel, num_steps_per_img=num_steps_per_img)\n",
    "\n",
    "            input_generator = InputGenerator(shape=(28,28), interval=num_steps_per_img)\n",
    "\n",
    "            input_generator.setBiasExp(bias_exp)\n",
    "\n",
    "\n",
    "            #set_verbosity(LoggingLevel.ERROR)\n",
    "\n",
    "            # Initialize empty model\n",
    "            snn_model = Model(\"dnn_model\")\n",
    "\n",
    "            # Add DNN and InputGenerator to empty model\n",
    "            snn_model.add(dnn)\n",
    "            snn_model.add(input_generator)\n",
    "\n",
    "\n",
    "            # Connect InputGenerator to DNN\n",
    "            # (Explicit)\n",
    "            # input_generator.ports.output.connect(dnn.ports.input)\n",
    "            # (Implicit when ports can be inferred)\n",
    "            input_generator.connect(dnn)\n",
    "\n",
    "            # Enfore particular execution order or processes/snips executing in the same phase\n",
    "            # (Here: Execute input injection as bias currents after network reset)\n",
    "            input_generator.processes.inputEncoder.executeAfter(dnn.processes.reset)\n",
    "\n",
    "            snn_model.compile()\n",
    "\n",
    "            if enable_energy_probe:\n",
    "                from nxsdk.api.enums.api_enums import ProbeParameter\n",
    "                from nxsdk.graph.monitor.probes import PerformanceProbeCondition\n",
    "                eProbe = snn_model.board.probe(probeType=ProbeParameter.ENERGY, \n",
    "                                               probeCondition=PerformanceProbeCondition(\n",
    "                                                    tStart=1, \n",
    "                                                    tEnd=num_test_samples*num_steps_per_img, \n",
    "                                                    bufferSize=1024, \n",
    "                                                    binSize=execution_time_probe_bin_size))\n",
    "\n",
    "            snn_model.start(snn_nxmodel.board)\n",
    "\n",
    "            tStart, tEndBoot, tEndInput, tEndClassification, classifications, labels = runModel(num_steps_per_img, \n",
    "                                                                                                x_test[:num_test_samples, :, :, 0], \n",
    "                                                                                                y_test[:num_test_samples])\n",
    "\n",
    "             # Runtime statistics\n",
    "            runtimeBoot = tEndBoot-tStart\n",
    "            runtimeInput = tEndInput-tStart\n",
    "            runtimeClassification = tEndClassification-tStart\n",
    "\n",
    "            vals_dense[0, i, k, j] = runtimeClassification-runtimeInput\n",
    "            vals_dense[3, i, k, j] = np.mean(eProbe.spikingTimePerTimeStep)*num_steps_per_img/1e3\n",
    "\n",
    "            # Accuracy statistics\n",
    "            errors = classifications != labels\n",
    "            num_errors = np.sum(errors)\n",
    "\n",
    "            vals_dense[1, i, k, j] = (num_test_samples-num_errors)/num_test_samples\n",
    "\n",
    "\n",
    "            vals_dense[2, i, k, j] = eProbe.totalEnergy\n",
    "            vals_dense[4, i, k, j] = np.mean(eProbe.totalEnergyPerTimeStep)*num_steps_per_img\n",
    "\n",
    "            if not measure_accuracy_runtime_trade_off:\n",
    "                snn_model.disconnect()\n",
    "\n",
    "    mean_vals_t1[:, k] = np.mean(vals_dense[0, :, k, :], 1)\n",
    "    std_vals_t1[:, k] = np.std(vals_dense[0, :, k, :], 1)\n",
    "\n",
    "    mean_vals_t2[:, k] = np.mean(vals_dense[3, :, k, :], 1)\n",
    "    std_vals_t2[:, k] = np.std(vals_dense[3, :, k, :], 1)\n",
    "\n",
    "    mean_vals_acc[:, k] = np.mean(vals_dense[1, :, k, :], 1)\n",
    "    std_vals_acc[:, k] = np.std(vals_dense[1, :, k, :], 1)\n",
    "\n",
    "    mean_vals_e1[:, k] = np.mean(vals_dense[2, :, k, :], 1)\n",
    "    std_vals_e1[:, k] = np.std(vals_dense[2, :, k, :], 1)\n",
    "\n",
    "    mean_vals_e2[:, k] = np.mean(vals_dense[4, :, k, :], 1)\n",
    "    std_vals_e2[:, k] = np.std(vals_dense[4, :, k, :], 1)\n",
    "\n",
    "total_list = [mean_vals_t1, std_vals_t1, mean_vals_t2, std_vals_t2, mean_vals_acc, \n",
    "              std_vals_acc, mean_vals_e1, std_vals_e1, mean_vals_e2, std_vals_e2]\n",
    "csv_names = [\"spike_mean_vals_t1.csv\", \"spike_std_vals_t1.csv\", \"spike_mean_vals_t2.csv\", 'spike_std_vals_t2.csv',\n",
    "             'spike_mean_vals_acc.csv', 'spike_std_vals_acc.csv', 'spike_mean_vals_e1.csv', 'spike_std_vals_e1.csv',\n",
    "             'spike_mean_vals_e2.csv', 'spike_std_vals_e2.csv']\n",
    "for i in range(len(csv_names)):\n",
    "    with open(csv_names[i], mode='w') as employee_file:\n",
    "        employee_writer = csv.writer(employee_file, delimiter=',',  quotechar='\"', \n",
    "                                     quoting=csv.QUOTE_MINIMAL)\n",
    "        employee_writer.writerow(total_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13096c1c-77f8-43c7-82c4-ec800530fedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-1, -1, -1, ...,  0,  0,  0],\n",
      "       [ 0,  0, -1, ...,  0,  0,  0],\n",
      "       [ 0,  1, -1, ...,  0,  0,  0],\n",
      "       ...,\n",
      "       [ 0,  0, -1, ...,  0, -1,  1],\n",
      "       [ 0,  0,  0, ...,  0,  0,  0],\n",
      "       [ 0,  1,  0, ...,  1,  0,  0]]), array([-28,  57,   5, -15,  34,  73, -23,  55, -71, -22])]\n"
     ]
    }
   ],
   "source": [
    "print(parameters_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c79d54-c0e5-4888-bfe5-5af1f919b6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "for i in range(len(step_sizes)):\n",
    "    plt.errorbar(layer_sizes, mean_vals_t1[:, i], yerr = std_vals_t1[:, i],\n",
    "                 label = f\"step_sizes = {step_sizes[i]}\")\n",
    "    plt.ylabel(\"time [s]\")\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.title(\"Total Time over hidden layers\")\n",
    "    plt.xscale('log')\n",
    "    plt.legend()\n",
    "    \n",
    "\n",
    "fig.savefig('total_time_dense_layer_stepsize.jpg', bbox_inches='tight', dpi=150)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8134c245-0173-4d72-80df-a5f4baa248c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "for i in range(len(step_sizes)):\n",
    "    plt.errorbar(layer_sizes, mean_vals_e1[:, i], yerr = std_vals_e1[:, i],\n",
    "                label = f\"step_sizes = {step_sizes[i]}\")\n",
    "    plt.xlabel(\"Hidden Layer size\")\n",
    "    plt.ylabel(\"energy [µJ]\")\n",
    "    plt.title(\"Total Energy over hidden layers\")\n",
    "    plt.xscale('log')\n",
    "    plt.legend()\n",
    "    \n",
    "fig.savefig('total_energy_dense_layer_stepsize.jpg', bbox_inches='tight', dpi=150)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3189c389",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "for i in range(len(step_sizes)):\n",
    "    plt.errorbar(layer_sizes, mean_vals_acc[:, i], yerr = std_vals_acc[:, i],\n",
    "                label = f\"step_sizes = {step_sizes[i]}\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"Hidden Layer size\")\n",
    "    plt.title(\"Accuracy over hidden layers\")\n",
    "    plt.xscale('log')\n",
    "\n",
    "fig.savefig('accuracy_dense_layer_stepsize.jpg', bbox_inches='tight', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "01302fc1-a3f0-4ee3-9cc8-57f2537b421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "for i in range(len(step_sizes)):\n",
    "    plt.errorbar(layer_sizes, mean_vals_t2[:, i], yerr = std_vals_t2[:, i],\n",
    "                 label = f\"step_sizes = {step_sizes[i]}\")\n",
    "    plt.xlabel(\"Hidden Layer size\")\n",
    "    plt.ylabel(\"Time [ms]\")\n",
    "    plt.title(\"Time per time step over hidden layers\")\n",
    "    plt.xscale('log')\n",
    "    plt.legend()\n",
    "    \n",
    "\n",
    "fig.savefig('time_timestep_av_dense_layer_stepsize.jpg', bbox_inches='tight', dpi=150)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf92881-1a54-41a6-8012-e28fe0e32e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "for i in range(len(step_sizes)):\n",
    "    plt.errorbar(layer_sizes, mean_vals_e2[:, i], yerr = std_vals_e2[:, i],\n",
    "                label = f\"step_sizes = {step_sizes[i]}\")\n",
    "    plt.xlabel(\"Hidden Layer size\")\n",
    "    plt.ylabel(\"Energy [µJ]\")\n",
    "    plt.title(\"Energy per time step over hidden layers\")\n",
    "    plt.xscale('log')\n",
    "    plt.legend()\n",
    "    \n",
    "fig.savefig('energy_timestep_av_dense_layer_stepsize.jpg', bbox_inches='tight', dpi=150)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6f7671f-21d5-4d68-b042-715180e90f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_time_ann = np.mean(ann_time, axis = 1)\n",
    "sd_time_ann = np.std(ann_time, axis = 1)\n",
    "\n",
    "mean_acc_ann = np.mean(accuracy_ANN, axis = 1)\n",
    "sd_acc_ann = np.std(accuracy_ANN, axis = 1)\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.plot(layer_sizes, mean_time_ann)\n",
    "plt.errorbar(layer_sizes, mean_time_ann, yerr = sd_time_ann, fmt ='o', mfc = 'b', mec = 'b', ms = 2)\n",
    "plt.ylabel(\"time[s]\")\n",
    "plt.xlabel(\"Hidden Layer size\")\n",
    "plt.xscale('log')\n",
    "\n",
    "fig.savefig('ANN_time_dense.jpg', bbox_inches='tight', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "021d8db1-3cde-4a61-acdd-0dff014a15b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.plot(layer_sizes, mean_acc_ann)\n",
    "plt.errorbar(layer_sizes, mean_acc_ann, yerr = sd_acc_ann, fmt ='o', mfc = 'b', mec = 'b', ms = 2)\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"Hidden Layer size\")\n",
    "plt.xscale('log')\n",
    "\n",
    "fig.savefig('ANN_acc_dense.jpg', bbox_inches='tight', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d49ba3e-9233-4e05-a408-3af9dce8c4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Throughput values \n",
    "through_time = 1/(mean_vals_t1/num_test_samples)\n",
    "through_time_sd = 1/(std_vals_t1/(num_test_samples)**2)\n",
    "\n",
    "through_energy = (mean_vals_e1/num_test_samples)\n",
    "through_energy_sd = 1/(std_vals_e1/(num_test_samples)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f1ff2d-d61b-422c-85ac-b4ba6ef12ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "for i in range(len(step_sizes)):\n",
    "    plt.errorbar(layer_sizes, through_time[:, i], yerr = through_time_sd[:, i],\n",
    "                 label = f\"step_sizes = {step_sizes[i]}\")\n",
    "    plt.xlabel(\"Hidden Layer size\")\n",
    "    plt.ylabel(\"Inferences per Second\")\n",
    "    plt.title(\"Inference Time over hidden layer size\")\n",
    "    plt.xscale('log')\n",
    "    plt.legend()\n",
    "    \n",
    "\n",
    "fig.savefig('Inference_time_dense_layer_stepsize.jpg', bbox_inches='tight', dpi=150)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d55225d-d920-45a5-b00e-841e0fe08f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "for i in range(len(step_sizes)):\n",
    "    plt.errorbar(layer_sizes, through_energy[:, i], yerr = through_energy_sd[:, i],\n",
    "                label = f\"step_sizes = {step_sizes[i]}\")\n",
    "    plt.xlabel(\"Hidden Layer size\")\n",
    "    plt.ylabel(\"Energy per Inference [µJ]\")\n",
    "    plt.title(\"Inference Energy over hidden layer size\")\n",
    "    plt.xscale('log')\n",
    "    plt.legend()\n",
    "    \n",
    "fig.savefig('Inference_energy_dense_layer_stepsize.jpg', bbox_inches='tight', dpi=150)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e71a83-2d1f-44fd-bd2e-c557c5cc0a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

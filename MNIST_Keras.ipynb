{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d39d69b-6242-4788-8189-ad4cdfc099ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "%matplotlib inline\n",
    "\n",
    "# Add path to the nxsdk_modules package.\n",
    "# sys.path.append('/homes/dcicch/models')\n",
    "# sys.path.append('/homes/dcicch/NxSDK_Package/nxsdk-apps')\n",
    "\n",
    "\n",
    "# Enable SLURM to run network on Loihi.\n",
    "os.environ['SLURM'] = '1'\n",
    "os.environ['PYTHONUNBUFFERED'] = '1'\n",
    "\n",
    "os.environ[\"PARTITION\"] = \"nahuku32\"\n",
    "os.environ[\"BOARD\"] = 'ncl-ext-ghrd-01'\n",
    "\n",
    "snipDir = os.path.abspath(os.path.join('..', 'snips', 'reset_model_states'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00be0921-3173-48a6-b841-44c538ca626a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_training_epochs = 2\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "num_steps_per_img = 512\n",
    "num_train_samples = 60000\n",
    "num_test_samples = 128\n",
    "\n",
    "# EnergyProbes allow to profile execution time, power and thus energy consumption\n",
    "enable_energy_probe = True\n",
    "execution_time_probe_bin_size = 512 # Small numbers slows down execution\n",
    "\n",
    "# Not yet supported\n",
    "measure_accuracy_runtime_trade_off = False # Not yet supported\n",
    "runtimes = [128, 256, 512, 1024, 2048]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9648da73-f6ce-4a2a-b266-0504be774976",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-06 03:34:45.271792: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-06 03:34:45.587084: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /slurm/intel-archi/lib\n",
      "2022-12-06 03:34:45.600345: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-06 03:34:47.443702: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /slurm/intel-archi/lib\n",
      "2022-12-06 03:34:47.443776: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /slurm/intel-archi/lib\n",
      "2022-12-06 03:34:47.443783: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Load standard MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize input so we can train ANN with it. \n",
    "# Will be converted back to integers for SNN layer.\n",
    "x_train = x_train[:num_train_samples, :, :] / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# Add a channel dimension.\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "# One-hot encode target vectors.\n",
    "y_train = np_utils.to_categorical(y_train[:num_train_samples], 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bf5b32d-4644-486e-9854-8708c93919d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-06 03:34:51.257003: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /slurm/intel-archi/lib\n",
      "2022-12-06 03:34:51.257029: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-06 03:34:51.257043: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ncl-edu): /proc/driver/nvidia/version does not exist\n",
      "2022-12-06 03:34:51.257236: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 12, 12, 16)        416       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 12, 12, 16)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 10, 10, 32)        4640      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 10, 10, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 4, 4, 64)          18496     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4, 4, 64)          0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 1, 1, 10)          10250     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33,802\n",
      "Trainable params: 33,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dropout, Flatten, Conv2D, Input\n",
    "    \n",
    "train_model = False\n",
    "    \n",
    "# Path for pre-trained model\n",
    "# Path for pre-trained model\n",
    "pretrained_model_path = os.path.join(os.path.abspath(''),\n",
    "                                     'models', \n",
    "                                     'a_minist_model.h5')\n",
    "    \n",
    "# Generate model\n",
    "if train_model or not os.path.isfile(pretrained_model_path):\n",
    "    # Define model\n",
    "    input_layer = Input(input_shape)\n",
    "\n",
    "    layer = Conv2D(filters=16, \n",
    "                   kernel_size=(5, 5), \n",
    "                   strides=(2, 2), \n",
    "                   input_shape=input_shape,\n",
    "                   activation='relu')(input_layer)\n",
    "    layer = Dropout(0.1)(layer)\n",
    "    layer = Conv2D(filters=32, \n",
    "                   kernel_size=(3, 3), \n",
    "                   activation='relu')(layer)\n",
    "    layer = Dropout(0.1)(layer)\n",
    "    layer = Conv2D(filters=64, \n",
    "                   kernel_size=(3, 3), \n",
    "                   strides=(2, 2), \n",
    "                   activation='relu')(layer)\n",
    "    layer = Dropout(0.1)(layer)\n",
    "    layer = Conv2D(filters=10, \n",
    "                   kernel_size=(4, 4), \n",
    "                   activation='softmax')(layer)\n",
    "    layer = Flatten()(layer)\n",
    "\n",
    "    ann_model = Model(input_layer, layer)\n",
    "\n",
    "    ann_model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Training \n",
    "    ann_model.fit(x_train, y_train, batch_size, num_training_epochs, verbose=2,\n",
    "          validation_data=(x_test, y_test))\n",
    "    \n",
    "    # Save model\n",
    "    ann_model.save(pretrained_model_path)\n",
    "else:\n",
    "    # Load pre-trained model\n",
    "    ann_model = keras.models.load_model(pretrained_model_path)\n",
    "    \n",
    "ann_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e1e809-eb82-419e-ad67-3d8be68b35ac",
   "metadata": {},
   "source": [
    "Creating Loihi Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ad93f9a-54fb-4127-a180-581cc97e3240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"nx_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (NxInputLayer)      (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      " nx_conv2d (NxConv2D)        (None, 12, 12, 16)        416       \n",
      "                                                                 \n",
      " nx_conv2d_1 (NxConv2D)      (None, 10, 10, 32)        4640      \n",
      "                                                                 \n",
      " nx_conv2d_2 (NxConv2D)      (None, 4, 4, 64)          18496     \n",
      "                                                                 \n",
      " nx_conv2d_3 (NxConv2D)      (None, 1, 1, 10)          10250     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33,802\n",
      "Trainable params: 33,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sys.path.append('/homes/lexjohan/Documents/models')\n",
    "os.path.join(\"nxsdk_modules_ncl/snntoolbox\")\n",
    "\n",
    "from nxsdk_modules_ncl.dnn.src.utils import extract\n",
    "from nxsdk_modules_ncl.dnn.src.dnn_layers import NxInputLayer, NxConv2D, \\\n",
    "    NxModel, ProbableStates\n",
    "\n",
    "vth_mant = 2**9\n",
    "bias_exp = 6\n",
    "weight_exponent = 0\n",
    "synapse_encoding = 'sparse'\n",
    "\n",
    "inputLayer = NxInputLayer(input_shape, \n",
    "                             vThMant=vth_mant, \n",
    "                             biasExp=bias_exp)\n",
    "        \n",
    "layer = NxConv2D(filters=16, \n",
    "                 kernel_size=(5, 5), \n",
    "                 strides=(2, 2), \n",
    "                 input_shape=input_shape,\n",
    "                 vThMant=vth_mant,\n",
    "                 weightExponent=weight_exponent,\n",
    "                 synapseEncoding=synapse_encoding)(inputLayer.input)\n",
    "layer = NxConv2D(filters=32, \n",
    "                 kernel_size=(3, 3), \n",
    "                 vThMant=vth_mant,\n",
    "                 weightExponent=weight_exponent,\n",
    "                 synapseEncoding=synapse_encoding)(layer)\n",
    "layer = NxConv2D(filters=64, \n",
    "                 kernel_size=(3, 3), \n",
    "                 strides=(2, 2), \n",
    "                 vThMant=vth_mant,\n",
    "                 weightExponent=weight_exponent,\n",
    "                 synapseEncoding=synapse_encoding)(layer)\n",
    "layer = NxConv2D(filters=10, \n",
    "                 kernel_size=(4, 4), \n",
    "                 activation='softmax', \n",
    "                 vThMant=vth_mant,\n",
    "                 weightExponent=weight_exponent,\n",
    "                 synapseEncoding=synapse_encoding)(layer)\n",
    "\n",
    "snn_nxmodel = NxModel(inputLayer.input, layer,\n",
    "                        numCandidatesToCompute=1)\n",
    "\n",
    "snn_nxmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20279fa2-8d1a-43e5-b595-e6ec82697926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nxsdk_modules_ncl.dnn.src.utils import to_integer\n",
    "\n",
    "# Extract weights and biases from parameter list.\n",
    "parameters = ann_model.get_weights()\n",
    "weights = parameters[0::2]\n",
    "biases = parameters[1::2]\n",
    "\n",
    "# Quantize weights and biases using max-normalization (Strong quantization loss if distributions have large tails)\n",
    "parameters_int = []\n",
    "for w, b in zip(weights, biases):\n",
    "    w_int, b_int = to_integer(w, b, 8)\n",
    "    parameters_int += [w_int, b_int]\n",
    "\n",
    "# Set quantized weigths and biases for spiking model\n",
    "snn_nxmodel.set_weights(parameters_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f50292e7-998e-4b0a-819d-c32400f2a56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotParamDist(params, params_int, max_val, num_bins=64):\n",
    "    num_layers = len(params)\n",
    "    for i, (p1, p2) in enumerate(zip(params, params_int)):\n",
    "        plt.subplot(1, num_layers, i+1)\n",
    "        scaled_p = np.ndarray.flatten(p1/max_val*255)\n",
    "        _, bins, _ = plt.hist(scaled_p, bins=num_bins, label='float (scaled)')\n",
    "        _ = plt.hist(np.ndarray.flatten(p2), bins=bins, alpha=0.5, label='int')\n",
    "        plt.title(\"Layer {}\".format(i+1))\n",
    "        plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "209d813b-a006-403d-87ed-23740a5fab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_int = parameters_int[0::2]\n",
    "biases_int = parameters_int[1::2]\n",
    "max_val = [np.max(np.abs(np.concatenate([w, b], None))) for w, b in zip(weights, biases)]\n",
    "max_val = np.max(max_val)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plotParamDist(weights, weights_int, max_val, num_bins=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb67e167-74f3-4eab-a77a-ad266782db8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plotParamDist(biases, biases_int, max_val, num_bins=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af15e04f-93db-4741-8e1a-1f76dafe0267",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/homes/lexjohan/Documents/models/')\n",
    "from nxsdk_modules_ncl.dnn.composable.composable_dnn import ComposableDNN as DNN\n",
    "from nxsdk_modules_ncl.input_generator.input_generator import InputGenerator\n",
    "\n",
    "# NxModel is not yet implemented as a Composable -> Wrap it with DNN composable class\n",
    "dnn = DNN(model=snn_nxmodel, num_steps_per_img=num_steps_per_img)\n",
    "\n",
    "input_generator = InputGenerator(shape=input_shape, interval=num_steps_per_img)\n",
    "\n",
    "input_generator.setBiasExp(bias_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29bec49c-1d03-4ccf-a4a1-35c40a8ee155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nxsdk.composable.model import Model\n",
    "from nxsdk.logutils.nxlogging import set_verbosity,LoggingLevel\n",
    "# set_verbosity(LoggingLevel.ERROR)\n",
    "\n",
    "# Initialize empty model\n",
    "snn_model = Model(\"dnn_model\")\n",
    "\n",
    "# Add DNN and InputGenerator to empty model\n",
    "snn_model.add(dnn)\n",
    "snn_model.add(input_generator)\n",
    "\n",
    "\n",
    "# Connect InputGenerator to DNN\n",
    "# (Explicit)\n",
    "# input_generator.ports.output.connect(dnn.ports.input)\n",
    "# (Implicit when ports can be inferred)\n",
    "input_generator.connect(dnn)\n",
    "\n",
    "# Enfore particular execution order or processes/snips executing in the same phase\n",
    "# (Here: Execute input injection as bias currents after network reset)\n",
    "input_generator.processes.inputEncoder.executeAfter(dnn.processes.reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c002ad97-449a-4979-9944-00898b707e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/lexjohan/nengoloihi/miniconda/lib/python3.9/site-packages/numpy/lib/arraysetops.py:272: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ar = np.asanyarray(ar)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....x\n",
      "\n",
      "...."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/lexjohan/Documents/models/nxsdk_modules_ncl/dnn/src/dnn_layers.py:437: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  permCxIdToRelCxId = np.array([np.where(permutedDestCxIdxs == i)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............x\n",
      "\n",
      "...x\n",
      "\n",
      "x\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<nxsdk.arch.n2a.n2board.N2Board at 0x7f4afc783460>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snn_model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a32dc078-9cfa-4faf-8b18-c75954dbf001",
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_energy_probe:\n",
    "    from nxsdk.api.enums.api_enums import ProbeParameter\n",
    "    from nxsdk.graph.monitor.probes import PerformanceProbeCondition\n",
    "    eProbe = snn_model.board.probe(\n",
    "        probeType=ProbeParameter.ENERGY, \n",
    "        probeCondition=PerformanceProbeCondition(\n",
    "            tStart=1, \n",
    "            tEnd=num_test_samples*num_steps_per_img, \n",
    "            bufferSize=1024, \n",
    "            binSize=execution_time_probe_bin_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "138fb12c-04b5-472c-afd3-e27f35886cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Encoding probes... sters... \r"
     ]
    }
   ],
   "source": [
    "snn_model.start(snn_nxmodel.board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "970a3538-6791-4d4f-a31b-346b692c4282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModel(num_steps_per_sample, x_test, y_test):\n",
    "    \"\"\"Runs the SNN Model to classify test images.\"\"\"\n",
    "      \n",
    "    # Initialize arrays for results\n",
    "    num_samples = len(y_test)\n",
    "    classifications = np.zeros(num_samples, int)\n",
    "    labels = np.zeros(num_samples, int)\n",
    "\n",
    "    # Run DNN to classify images\n",
    "    tStart = time.time()\n",
    "    snn_model.run(num_steps_per_sample * num_samples, aSync=True)\n",
    "    tEndBoot = time.time()\n",
    "\n",
    "    # Enqueue images by pushing them into InputGenerator\n",
    "    print(\"Queuing images...\")\n",
    "    for i, (x, y) in enumerate(zip((x_test* 255).astype(int), y_test)):\n",
    "        input_generator.encode(x)\n",
    "        labels[i] = np.argmax(y)\n",
    "    tEndInput = time.time()\n",
    "\n",
    "    # Read out classification results for all images\n",
    "    print(\"Waiting for classification to finish...\")\n",
    "    classifications = list(dnn.readout_channel.read(num_samples))\n",
    "    tEndClassification = time.time()\n",
    "    \n",
    "    # finishRun fetches EnergyProbe data if configured\n",
    "    snn_model.finishRun()\n",
    "    \n",
    "    return tStart, tEndBoot, tEndInput, tEndClassification, classifications, labels\n",
    "\n",
    "def calcAccuracy(classifications, labels):\n",
    "    \"\"\"Computes classification accuracy for a set of images given classification and labels.\"\"\"\n",
    "    errors = classifications != labels\n",
    "    num_errors = np.sum(errors)\n",
    "    num_samples = len(classifications)\n",
    "    return (num_samples-num_errors)/num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0a77930-561e-484e-8e1a-dd944dc78702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queuing images...spikes... . \n",
      "Waiting for classification to finish...\n",
      "    Processing timeseries... \r"
     ]
    }
   ],
   "source": [
    "tStart, tEndBoot, tEndInput, tEndClassification, classifications, labels = runModel(num_steps_per_img, \n",
    "                                                                                    x_test[:num_test_samples, :, :], \n",
    "                                                                                    y_test[:num_test_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d62693b8-30c3-4ec9-9c07-bc8f7c8c7e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not measure_accuracy_runtime_trade_off:\n",
    "    snn_model.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "edb0687f-4a7d-4d90-8b9f-204be3903bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_samples(samples, guesses, lables, numCols=5, size=28):\n",
    "    \"\"\"Plots samples as an array of images.\"\"\"\n",
    "    \n",
    "    import math\n",
    "    numSamples = len(samples)\n",
    "    numRows = int(math.ceil(numSamples/numCols))\n",
    "    plt.figure(3, figsize=(20, 10))\n",
    "    i = 0\n",
    "    for c in range(numCols):\n",
    "        for r in range(numRows):\n",
    "            plt.subplot(numRows, numCols, i+1)\n",
    "            plt.imshow(np.reshape(samples[i,:], (size, size)))\n",
    "            plt.axis('off')\n",
    "            plt.title('C:{}/L:{}'.format(guesses[i], lables[i]))\n",
    "            i += 1\n",
    "            if i == numSamples:\n",
    "                break\n",
    "        if i == numSamples:\n",
    "            break\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1cb6fdea-ed0e-42e2-9d56-cada8f2d7f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime statistics including boot, input generation and classification:\n",
      "-----------------------------------------------------------------------\n",
      "    Runtime till end of boot: 1.487 s\n",
      "    Runtime till end of input queueing: 2.925 s\n",
      "    Runtime till end of classification: 4.815 s\n",
      "    Effective average time per time step: 73.476us\n",
      "    Effective average classification time per sample: 37.620ms\n",
      "\n",
      "Accuracy metrics:\n",
      "    Number of incorrect classifications: 1 of 128\n",
      "    Classification accuracy: 99.22%\n",
      "\n",
      "Some correct classifications:\n",
      "Incorrect classifications:\n"
     ]
    }
   ],
   "source": [
    "# Runtime statistics\n",
    "runtimeBoot = tEndBoot-tStart\n",
    "runtimeInput = tEndInput-tStart\n",
    "runtimeClassification = tEndClassification-tStart\n",
    "print(\"Runtime statistics including boot, input generation and classification:\")\n",
    "print(\"-----------------------------------------------------------------------\")\n",
    "print(\"    Runtime till end of boot: {:.3f} s\".format(runtimeBoot))\n",
    "print(\"    Runtime till end of input queueing: {:.3f} s\".format(runtimeInput))\n",
    "print(\"    Runtime till end of classification: {:.3f} s\".format(runtimeClassification))\n",
    "print(\"    Effective average time per time step: %.3fus\"%(runtimeClassification*1e6/(num_test_samples*num_steps_per_img)))\n",
    "print(\"    Effective average classification time per sample: %.3fms\"%(runtimeClassification*1e3/num_test_samples))\n",
    "print(\"\")\n",
    "\n",
    "# Accuracy statistics\n",
    "errors = classifications != labels\n",
    "num_errors = np.sum(errors)\n",
    "print(\"Accuracy metrics:\")\n",
    "print(\"    Number of incorrect classifications: {} of {}\".format(num_errors, num_test_samples))\n",
    "print(\"    Classification accuracy: {:.2%}\".format((num_test_samples-num_errors)/num_test_samples))\n",
    "print(\"\")\n",
    "\n",
    "# Show some correctly classified images\n",
    "maxShow = 15\n",
    "if num_errors < num_test_samples:\n",
    "    print(\"Some correct classifications:\")\n",
    "    correct_idx = np.where(errors==0)[0]\n",
    "    correct_idx = correct_idx[:min([maxShow, len(correct_idx)])]\n",
    "    plot_samples(\n",
    "        samples=x_test[correct_idx], \n",
    "        guesses=[classifications[i] for i in correct_idx], \n",
    "        lables=[labels[i] for i in correct_idx], \n",
    "        numCols=15)\n",
    "\n",
    "# Show incorrectly classified images\n",
    "if num_errors > 0:\n",
    "    print(\"Incorrect classifications:\")\n",
    "    error_idx = np.where(errors)[0]\n",
    "    error_idx = error_idx[:min([maxShow, len(correct_idx)])]\n",
    "    plot_samples(\n",
    "        samples=x_test[error_idx], \n",
    "        guesses=[classifications[i] for i in error_idx], \n",
    "        lables=[labels[i] for i in error_idx], \n",
    "        numCols=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "920ab621-afb0-4592-b8ef-a4750d80449a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime statistics including input generation and classification:\n",
      "-----------------------------------------------------------------\n",
      "Average time per time step: 50.340us\n",
      "Average spiking time per time step: 49.459us\n",
      "Average management time per time step: 0.881us\n",
      "Average classification time per sample: 25.323ms\n",
      "Average I/O time per sample: 0.451ms\n"
     ]
    }
   ],
   "source": [
    "if enable_energy_probe:\n",
    "    import numpy as np\n",
    "    print(\"Runtime statistics including input generation and classification:\")\n",
    "    print(\"-----------------------------------------------------------------\")\n",
    "    print(\"Average time per time step: %.3fus\"%(np.mean(eProbe.totalTimePerTimeStep)))\n",
    "    print(\"Average spiking time per time step: %.3fus\"%(np.mean(eProbe.spikingTimePerTimeStep)))\n",
    "    print(\"Average management time per time step: %.3fus\"%(np.mean(eProbe.managementTimePerTimeStep)))\n",
    "    print(\"Average classification time per sample: %.3fms\"%(np.mean(eProbe.spikingTimePerTimeStep)*num_steps_per_img/1e3))\n",
    "    print(\"Average I/O time per sample: %.3fms\"%(np.mean(eProbe.managementTimePerTimeStep)*num_steps_per_img/1e3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45c42abf-196d-4927-b42b-c97dd1b3b610",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_img_to_plot = 8\n",
    "\n",
    "# if enable_energy_probe:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "eProbe.plotExecutionTime()\n",
    "plt.xlim(0, num_steps_per_img*num_img_to_plot)\n",
    "plt.gca().set_yscale('log')\n",
    "plt.legend(loc='upper center')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(eProbe.spikingTimePerTimeStep, bins=64)\n",
    "plt.xlabel('Spike time per time step (us)')\n",
    "plt.title(\"Spike time per time step histogram\")\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(eProbe.managementTimePerTimeStep, bins=64)\n",
    "plt.gca().set_xscale('log')\n",
    "plt.xlabel('Management time per time step (us)')\n",
    "plt.title(\"Management time per time step historgram\")\n",
    "plt.show()\n",
    "\n",
    "# Note: Power measurements won't be accurate for small binSizes due to frequent executionTimeBuffer transfers\n",
    "plt.figure(figsize=(15, 5))\n",
    "eProbe.plotPower()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "eProbe.plotEnergy()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6c440e7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can not put single artist in more than one figure",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39m# plot = plots[-1]\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# plt.plot(plots[-1])\u001b[39;00m\n\u001b[1;32m      4\u001b[0m fig, ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots()\n\u001b[0;32m----> 6\u001b[0m ax\u001b[39m.\u001b[39;49madd_collection(plots[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])\n",
      "File \u001b[0;32m~/nengoloihi/miniconda/lib/python3.9/site-packages/matplotlib/axes/_base.py:2054\u001b[0m, in \u001b[0;36m_AxesBase.add_collection\u001b[0;34m(self, collection, autolim)\u001b[0m\n\u001b[1;32m   2052\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcollections\u001b[39m.\u001b[39mappend(collection)\n\u001b[1;32m   2053\u001b[0m collection\u001b[39m.\u001b[39m_remove_method \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcollections\u001b[39m.\u001b[39mremove\n\u001b[0;32m-> 2054\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_artist_props(collection)\n\u001b[1;32m   2056\u001b[0m \u001b[39mif\u001b[39;00m collection\u001b[39m.\u001b[39mget_clip_path() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2057\u001b[0m     collection\u001b[39m.\u001b[39mset_clip_path(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch)\n",
      "File \u001b[0;32m~/nengoloihi/miniconda/lib/python3.9/site-packages/matplotlib/axes/_base.py:1091\u001b[0m, in \u001b[0;36m_AxesBase._set_artist_props\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_artist_props\u001b[39m(\u001b[39mself\u001b[39m, a):\n\u001b[1;32m   1090\u001b[0m     \u001b[39m\"\"\"Set the boilerplate props for artists added to axes.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1091\u001b[0m     a\u001b[39m.\u001b[39;49mset_figure(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure)\n\u001b[1;32m   1092\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m a\u001b[39m.\u001b[39mis_transform_set():\n\u001b[1;32m   1093\u001b[0m         a\u001b[39m.\u001b[39mset_transform(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransData)\n",
      "File \u001b[0;32m~/nengoloihi/miniconda/lib/python3.9/site-packages/matplotlib/artist.py:730\u001b[0m, in \u001b[0;36mArtist.set_figure\u001b[0;34m(self, fig)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[39m# if we currently have a figure (the case of both `self.figure`\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[39m# and *fig* being none is taken care of above) we then user is\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[39m# trying to change the figure an artist is associated with which\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[39m# is not allowed for the same reason as adding the same instance\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[39m# to more than one Axes\u001b[39;00m\n\u001b[1;32m    729\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 730\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCan not put single artist in \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    731\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mmore than one figure\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    732\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure \u001b[39m=\u001b[39m fig\n\u001b[1;32m    733\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can not put single artist in more than one figure"
     ]
    }
   ],
   "source": [
    "plots = eProbe.plotPower()\n",
    "# plot = plots[-1]\n",
    "# plt.plot(plots[-1])\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.add_collection(plots[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb747f1d-66be-4680-960b-839a2ea7cbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Energy used: 2813171.298946436uJ\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Energy used: {eProbe.totalEnergy}{eProbe.energyUnits}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df36070-61a2-44a4-834c-0d1e5d3d6c55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  5 2022, 06:56:58) \n[GCC 7.5.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "0c9fdf67ddc2e2392292f776e8d08f045c7d7e879e644d5d895c727cdf27b2ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
